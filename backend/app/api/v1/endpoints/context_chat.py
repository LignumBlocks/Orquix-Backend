import logging
import time
from datetime import datetime
from typing import List, Optional, Dict, Any
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_db
from app.api.v1.endpoints.auth import get_current_user
from app.schemas.auth import SessionUser
from app.crud import context_session as context_crud
from app.crud import ia_prompt as ia_prompt_crud
from app.models.context_session import (
    ContextChatRequest,
    ContextChatResponse,
    ContextFinalizeRequest,
    ContextMessage,
    ContextSession,
    ContextSessionSummary
)
from app.schemas.ia_prompt import (
    GeneratePromptRequest,
    GeneratePromptResponse,
    ExecutePromptRequest,
    IAPromptResponse,
    IAPromptUpdate
)
from app.services.context_builder import context_builder_service
from app.services.prompt_templates import PromptTemplateManager
from app.schemas.ai_response import AIProviderEnum

logger = logging.getLogger(__name__)

router = APIRouter()

async def _automatically_include_moderator_synthesis(
    db: AsyncSession,
    project_id: UUID,
    user_id: UUID,
    current_context: str
) -> str:
    """
    Busca autom√°ticamente s√≠ntesis del moderador reciente y la incluye en el contexto.
    
    Args:
        db: Sesi√≥n de base de datos
        project_id: ID del proyecto
        user_id: ID del usuario
        current_context: Contexto actual acumulado
        
    Returns:
        Contexto mejorado con s√≠ntesis del moderador (si est√° disponible)
    """
    try:
        from sqlalchemy import select, and_, desc
        from app.models.models import InteractionEvent
        import json
        
        # Buscar la interacci√≥n m√°s reciente con s√≠ntesis del moderador
        stmt = select(InteractionEvent).where(
            and_(
                InteractionEvent.project_id == project_id,
                InteractionEvent.user_id == user_id,
                InteractionEvent.moderator_synthesis_json.isnot(None),
                InteractionEvent.deleted_at.is_(None)
            )
        ).order_by(desc(InteractionEvent.created_at)).limit(1)
        
        result = await db.execute(stmt)
        recent_interaction = result.scalar_one_or_none()
        
        if not recent_interaction:
            # No hay s√≠ntesis del moderador disponible
            logger.debug(f"No se encontr√≥ s√≠ntesis del moderador para proyecto {project_id}")
            return current_context
        
        # Verificar si la s√≠ntesis ya est√° incluida en el contexto
        if "üî¨ An√°lisis del Moderador IA" in current_context:
            logger.debug("S√≠ntesis del moderador ya incluida en el contexto")
            return current_context
        
        # Parsear la s√≠ntesis del moderador
        moderator_data = json.loads(recent_interaction.moderator_synthesis_json)
        
        logger.info(f"‚ú® Incluyendo autom√°ticamente s√≠ntesis del moderador - Calidad: {moderator_data.get('quality', 'unknown')}")
        
        # Incluir la s√≠ntesis en el contexto usando el m√©todo del context builder
        enhanced_context = context_builder_service.include_moderator_synthesis(
            current_context=current_context,
            synthesis_text=moderator_data.get('synthesis_text', ''),
            key_themes=moderator_data.get('key_themes', []),
            recommendations=moderator_data.get('recommendations', [])
        )
        
        logger.info(f"üß† Contexto enriquecido autom√°ticamente - Anterior: {len(current_context)} chars, Nuevo: {len(enhanced_context)} chars")
        
        return enhanced_context
        
    except Exception as e:
        logger.warning(f"Error incluyendo s√≠ntesis del moderador autom√°ticamente: {e}")
        # En caso de error, devolver el contexto original
        return current_context

def require_auth(current_user: Optional[SessionUser] = Depends(get_current_user)) -> SessionUser:
    """Require authentication for protected endpoints."""
    if not current_user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authentication required"
        )
    return current_user


@router.post(
    "/projects/{project_id}/context-chat",
    response_model=ContextChatResponse,
    summary="Chat para construcci√≥n de contexto",
    description="Endpoint para chatear y construir contexto antes de enviar a las IAs principales"
)
async def context_chat(
    project_id: UUID,
    request: ContextChatRequest,
    db: AsyncSession = Depends(get_db),
    current_user: SessionUser = Depends(require_auth)
):
    """
    Procesa un mensaje del usuario en la construcci√≥n de contexto.
    
    - Si no hay session_id, crea una nueva sesi√≥n
    - Si hay session_id, contin√∫a la conversaci√≥n existente
    - Usa GPT-3.5 para generar respuestas conversacionales
    """
    try:
        user_id = UUID(current_user.id)  # Convertir string a UUID
        logger.info(f"üí¨ Context chat - Proyecto: {project_id}, Usuario: {user_id}")
        logger.info(f"üí¨ Tipos - project_id: {type(project_id)}, user_id: {type(user_id)}")
        
        # Obtener o crear sesi√≥n
        if request.session_id:
            # Continuar sesi√≥n existente
            # Convertir expl√≠citamente a UUID si viene como string
            session_uuid = request.session_id
            if isinstance(session_uuid, str):
                session_uuid = UUID(session_uuid)
            
            logger.info(f"üîç Buscando sesi√≥n existente: {session_uuid} (tipo: {type(session_uuid)})")
            db_session = await context_crud.get_context_session(db, session_uuid)
            logger.info(f"üìã Sesi√≥n encontrada: {db_session is not None}")
            
            # Validar que la sesi√≥n pertenece al proyecto y usuario correctos
            if db_session:
                logger.info(f"üìã Sesi√≥n details: project_id={db_session.project_id}, user_id={db_session.user_id}, status={db_session.session_status}")
                logger.info(f"üìã Comparaci√≥n project_id: {db_session.project_id} == {project_id} -> {db_session.project_id == project_id}")
                logger.info(f"üìã Comparaci√≥n user_id: {db_session.user_id} == {user_id} -> {db_session.user_id == user_id}")
                
                # Si la sesi√≥n no pertenece al proyecto/usuario actual, crear una nueva
                if db_session.project_id != project_id or db_session.user_id != user_id:
                    logger.warning(f"‚ö†Ô∏è Sesi√≥n {session_uuid} pertenece a otro proyecto/usuario. Creando nueva sesi√≥n.")
                    db_session = await context_crud.create_context_session(
                        db=db,
                        project_id=project_id,
                        user_id=user_id,
                        initial_message=request.user_message
                    )
            else:
                # Sesi√≥n no encontrada, crear nueva
                logger.info(f"üÜï Sesi√≥n {session_uuid} no encontrada. Creando nueva sesi√≥n.")
                db_session = await context_crud.create_context_session(
                    db=db,
                    project_id=project_id,
                    user_id=user_id,
                    initial_message=request.user_message
                )
        else:
            # Crear nueva sesi√≥n
            logger.info(f"üÜï Creando nueva sesi√≥n para proyecto {project_id}")
            db_session = await context_crud.create_context_session(
                db=db,
                project_id=project_id,
                user_id=user_id,
                initial_message=request.user_message
            )
        
        # Convertir a modelo Pydantic para trabajar con el servicio
        session = context_crud.convert_interaction_to_context_session(db_session)
        
        # Buscar s√≠ntesis del moderador reciente y agregar autom√°ticamente al contexto
        enhanced_context = await _automatically_include_moderator_synthesis(
            db=db,
            project_id=project_id,
            user_id=user_id,
            current_context=session.accumulated_context
        )
        
        # Procesar mensaje con GPT-3.5 usando el contexto mejorado
        response = await context_builder_service.process_user_message(
            user_message=request.user_message,
            conversation_history=session.conversation_history,
            current_context=enhanced_context
        )
        
        # Actualizar session_id con el real
        response.session_id = db_session.id
        
        # Crear mensajes para actualizar la sesi√≥n
        user_message = ContextMessage(
            role="user",
            content=request.user_message,
            timestamp=datetime.utcnow(),
            message_type="information" if response.message_type == "information" else "question"
        )
        
        ai_message = ContextMessage(
            role="assistant",
            content=response.ai_response,
            timestamp=datetime.utcnow(),
            message_type=response.message_type
        )
        
        # Actualizar sesi√≥n con mensaje del usuario
        await context_crud.update_context_session(
            db=db,
            session=db_session,
            new_message=user_message,
            updated_context=response.accumulated_context
        )
        
        # Actualizar sesi√≥n con respuesta de la IA
        await context_crud.update_context_session(
            db=db,
            session=db_session,
            new_message=ai_message,
            updated_context=response.accumulated_context
        )
        
        logger.info(f"‚úÖ Context chat procesado - Sesi√≥n: {response.session_id}")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Error en context chat: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error procesando mensaje: {str(e)}"
        )


@router.get(
    "/projects/{project_id}/context-sessions",
    response_model=List[ContextSessionSummary],
    summary="Listar sesiones de contexto",
    description="Obtiene las sesiones de construcci√≥n de contexto de un proyecto"
)
async def get_context_sessions(
    project_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_user: SessionUser = Depends(require_auth)
):
    """
    Obtiene las sesiones de contexto de un proyecto.
    """
    try:
        user_id = UUID(current_user.id)  # Convertir string a UUID
        sessions = await context_crud.get_project_context_sessions(
            db=db,
            project_id=project_id,
            user_id=user_id
        )
        return sessions
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo sesiones de contexto: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo sesiones: {str(e)}"
        )


@router.get(
    "/context-sessions/{session_id}",
    response_model=ContextSession,
    summary="Obtener sesi√≥n de contexto",
    description="Obtiene una sesi√≥n de contexto espec√≠fica con todo su historial"
)
async def get_context_session(
    session_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_user: SessionUser = Depends(require_auth)
):
    """
    Obtiene una sesi√≥n de contexto espec√≠fica.
    """
    try:
        user_id = UUID(current_user.id)  # Convertir string a UUID
        db_session = await context_crud.get_context_session(db, session_id)
        if not db_session or db_session.user_id != user_id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sesi√≥n de contexto no encontrada"
            )
        
        # Convertir a modelo Pydantic para acceder a los campos correctamente
        session_model = context_crud.convert_interaction_to_context_session(db_session)
        return session_model
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo sesi√≥n de contexto: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo sesi√≥n: {str(e)}"
        )


@router.post(
    "/context-sessions/{session_id}/finalize",
    summary="Finalizar construcci√≥n de contexto",
    description="Finaliza la construcci√≥n de contexto y env√≠a la consulta a las IAs principales"
)
async def finalize_context_session(
    session_id: UUID,
    request: ContextFinalizeRequest,
    db: AsyncSession = Depends(get_db),
    current_user: SessionUser = Depends(require_auth)
):
    """
    Finaliza la construcci√≥n de contexto y env√≠a la consulta a las IAs principales.
    
    Este endpoint:
    1. Marca la sesi√≥n como finalizada
    2. Toma el contexto acumulado + pregunta final
    3. Lo env√≠a al flujo normal de IAs principales
    """
    try:
        user_id = UUID(current_user.id)  # Convertir string a UUID
        logger.info(f"üèÅ Finalizando sesi√≥n de contexto: {session_id}")
        
        # Obtener y validar sesi√≥n
        db_session = await context_crud.get_context_session(db, session_id)
        if not db_session or db_session.user_id != user_id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sesi√≥n de contexto no encontrada"
            )
        
        if db_session.session_status != "active":
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="La sesi√≥n ya est√° finalizada"
            )
        
        # Finalizar sesi√≥n
        await context_crud.finalize_context_session(db, session_id)
        
        # TODO: Aqu√≠ se integrar√≠a con el flujo normal de IAs principales
        # Por ahora retornamos informaci√≥n de la finalizaci√≥n
        
        return {
            "message": "Sesi√≥n de contexto finalizada exitosamente",
            "session_id": session_id,
            "accumulated_context": db_session.context_used_summary,
            "final_question": request.final_question,
            "ready_for_ai_processing": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error finalizando sesi√≥n de contexto: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error finalizando sesi√≥n: {str(e)}"
        )


@router.get(
    "/projects/{project_id}/active-context-session",
    response_model=ContextSession,
    summary="Obtener sesi√≥n activa",
    description="Obtiene la sesi√≥n de contexto activa para un proyecto"
)
async def get_active_context_session(
    project_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_user: SessionUser = Depends(require_auth)
):
    """
    Obtiene la sesi√≥n de contexto activa para un proyecto.
    """
    try:
        user_id = UUID(current_user.id)  # Convertir string a UUID
        db_session = await context_crud.get_active_session_for_project(
            db=db,
            project_id=project_id,
            user_id=user_id
        )
        
        if not db_session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No hay sesi√≥n de contexto activa"
            )
        
        session = context_crud.convert_interaction_to_context_session(db_session)
        return session
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo sesi√≥n activa: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo sesi√≥n activa: {str(e)}"
        )


@router.post("/context-sessions/{session_id}/generate-ai-prompts")
async def generate_ai_prompts(
    session_id: UUID,
    request: ContextFinalizeRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Genera los prompts reales usando el query_service y prompt_templates.
    Muestra exactamente los prompts que se enviar√≠an a cada IA.
    """
    logger.info(f"üéØ Generando prompts usando query_service - Sesi√≥n: {session_id}")
    
    try:
        # Verificar que la sesi√≥n existe y pertenece al usuario
        session = await context_crud.get_context_session(db, session_id)
        if not session:
            raise HTTPException(
                status_code=404,
                detail="Sesi√≥n de contexto no encontrada"
            )
        
        user_id = UUID(current_user.id)  # Convertir string a UUID
        if session.user_id != user_id:
            raise HTTPException(
                status_code=403,
                detail="No tienes permiso para acceder a esta sesi√≥n"
            )
        
        # Convertir a modelo Pydantic para acceder a los campos correctamente
        session_model = context_crud.convert_interaction_to_context_session(session)
        
        # Importar los servicios correctos
        from app.services.query_service import QueryService
        from app.services.prompt_templates import PromptTemplateManager
        from app.schemas.ai_response import AIProviderEnum
        from app.schemas.query import QueryRequest, QueryType
        from app.core.config import settings
        
        # Crear instancias de los servicios
        query_service = QueryService()
        prompt_manager = PromptTemplateManager()
        
        # Determinar proveedores disponibles
        available_providers = []
        if settings.OPENAI_API_KEY:
            available_providers.append(AIProviderEnum.OPENAI)
        if settings.ANTHROPIC_API_KEY:
            available_providers.append(AIProviderEnum.ANTHROPIC)
        
        if not available_providers:
            raise HTTPException(
                status_code=500,
                detail="No hay proveedores de IA configurados"
            )
        
        # Crear una QueryRequest simulada para obtener los prompts
        query_request = QueryRequest(
            user_question=request.final_question,
            query_type=QueryType.CONTEXT_AWARE,
            user_id=user_id,
            project_id=session.project_id,
            max_tokens=1200,
            temperature=0.7
        )
        
        # Usar el contexto acumulado de la sesi√≥n
        context_text = session_model.accumulated_context or ""
        
        # Generar prompts para cada proveedor usando el sistema oficial
        ai_prompts = {}
        
        for provider in available_providers:
            try:
                # Usar el m√©todo oficial del query_service para construir prompts
                prompt_data = prompt_manager.build_prompt_for_provider(
                    provider=provider,
                    user_question=query_request.user_question,
                    context_text=context_text,
                    additional_vars={
                        'timestamp': datetime.utcnow().isoformat(),
                        'project_name': f"Proyecto-{str(session.project_id)[:8]}"
                    }
                )
                
                # Optimizar el prompt para el proveedor
                optimized_prompt = prompt_manager.optimize_prompt_for_provider(
                    provider, prompt_data, query_request.max_tokens
                )
                
                # Obtener modelo espec√≠fico del proveedor
                if provider == AIProviderEnum.OPENAI:
                    model = "gpt-4o-mini"
                elif provider == AIProviderEnum.ANTHROPIC:
                    model = "claude-3-haiku-20240307"
                else:
                    model = "unknown"
                
                ai_prompts[provider.value.lower()] = {
                    "provider": f"{provider.value.title()}",
                    "model": model,
                    "system_message": optimized_prompt['system_message'],
                    "user_prompt": optimized_prompt['user_message'],
                    "parameters": {
                        "max_tokens": query_request.max_tokens,
                        "temperature": query_request.temperature
                    },
                    "template_used": "prompt_templates_v2.0"
                }
                
            except Exception as e:
                logger.error(f"Error generando prompt para {provider}: {e}")
                ai_prompts[provider.value.lower()] = {
                    "provider": f"{provider.value.title()}",
                    "error": f"Error generando prompt: {str(e)}"
                }
        
        logger.info(f"‚úÖ Prompts generados usando query_service para {len(ai_prompts)} proveedores")
        
        return {
            "session_id": session_id,
            "ai_prompts": ai_prompts,
            "prompt_system": "query_service + prompt_templates",
            "context_used": context_text[:200] + "..." if len(context_text) > 200 else context_text,
            "final_question": request.final_question
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error generando prompts con query_service: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generando prompts: {str(e)}"
        )


@router.post("/context-sessions/{session_id}/query-ais")
async def query_ais_individually(
    session_id: UUID,
    request: ContextFinalizeRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Endpoint espec√≠fico para enviar los prompts a las IAs y obtener respuestas individuales.
    Se ejecuta despu√©s de generar los prompts.
    """
    logger.info(f"ü§ñ Consultando IAs individualmente - Sesi√≥n: {session_id}")
    
    try:
        # Verificar que la sesi√≥n existe y pertenece al usuario
        session = await context_crud.get_context_session(db, session_id)
        if not session:
            raise HTTPException(
                status_code=404,
                detail="Sesi√≥n de contexto no encontrada"
            )
        
        user_id = UUID(current_user.id)
        if session.user_id != user_id:
            raise HTTPException(
                status_code=403,
                detail="No tienes permiso para acceder a esta sesi√≥n"
            )
        
        # Convertir a modelo Pydantic
        session_model = context_crud.convert_interaction_to_context_session(session)
        
        # Importar servicios necesarios
        from app.services.ai_orchestrator import AIOrchestrator
        from app.schemas.ai_response import AIRequest, AIProviderEnum
        from app.core.config import settings
        import time
        
        # PASO 1: Generar prompts autom√°ticamente usando PromptTemplateManager
        from app.services.prompt_templates import PromptTemplateManager
        prompt_manager = PromptTemplateManager()
        
        logger.info(f"üéØ Generando prompts autom√°ticamente usando prompt_manager - Sesi√≥n: {session_id}")
        
        # Generar prompts para cada proveedor
        prompts_by_provider = {}
        context_text = session_model.accumulated_context or ""
        
        for provider in [AIProviderEnum.OPENAI, AIProviderEnum.ANTHROPIC]:
            try:
                prompt_dict = prompt_manager.build_prompt_for_provider(
                    provider=provider,
                    user_question=request.final_question,
                    context_text=context_text
                )
                # Combinar system y user message en un solo prompt
                full_prompt = f"{prompt_dict['system_message']}\n\n{prompt_dict['user_message']}"
                
                prompts_by_provider[provider.value.lower()] = {
                    'prompt': full_prompt,
                    'system_message': prompt_dict['system_message'],
                    'user_message': prompt_dict['user_message']
                }
            except Exception as e:
                logger.warning(f"Error generando prompt para {provider.value}: {e}")
                # Fallback b√°sico
                fallback_prompt = f"Responde la siguiente pregunta en espa√±ol bas√°ndote en el contexto proporcionado:\n\nContexto: {context_text}\n\nPregunta: {request.final_question}"
                prompts_by_provider[provider.value.lower()] = {
                    'prompt': fallback_prompt,
                    'system_message': "Eres un asistente √∫til que responde en espa√±ol.",
                    'user_message': fallback_prompt
                }
        
        # Crear estructura compatible con el c√≥digo existente
        prompts_result = {
            'prompts_by_provider': prompts_by_provider,
            'context_summary': context_text[:500] + "..." if len(context_text) > 500 else context_text
        }
        
        logger.info(f"‚úÖ Prompts generados para {len(prompts_by_provider)} proveedores")
        
        # PASO 2: Crear instancia del orquestador para enviar prompts
        orchestrator = AIOrchestrator()
        
        # Usar los prompts generados en lugar de crear uno gen√©rico
        prompts_by_provider = prompts_result.get('prompts_by_provider', {})
        
        # Ejecutar consultas individuales a las IAs
        start_time = time.time()
        individual_responses = []
        
        # Obtener proveedores disponibles
        available_providers = orchestrator.get_available_providers()
        
        # Consultar cada proveedor individualmente usando sus prompts espec√≠ficos
        for provider in available_providers:
            provider_name = provider.value.lower()
            logger.info(f"ü§ñ Consultando {provider.value.title()}...")
            provider_start = time.time()
            
            # Obtener el prompt espec√≠fico para este proveedor
            provider_prompt_data = prompts_by_provider.get(provider_name, {})
            if not provider_prompt_data:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ prompt para {provider.value.title()}, saltando...")
                continue
            
            prompt_text = provider_prompt_data.get('prompt', '')
            if not prompt_text:
                logger.warning(f"‚ö†Ô∏è Prompt vac√≠o para {provider.value.title()}, saltando...")
                continue
            
            try:
                # Crear AIRequest espec√≠fico para este proveedor
                ai_request = AIRequest(
                    prompt=prompt_text,
                    max_tokens=1200,
                    temperature=0.7,
                    user_id=str(user_id),
                    project_id=str(session.project_id)
                )
                
                ai_response = await orchestrator.generate_single_response(
                    request=ai_request,
                    provider=provider
                )
                provider_time = time.time() - provider_start
                
                # Determinar modelo seg√∫n el proveedor
                model = "gpt-4o-mini" if provider == AIProviderEnum.OPENAI else "claude-3-haiku-20240307"
                
                if ai_response.status.value == "success":
                    # PASO 3: Guardar prompt y respuesta en BD (sin bloquear si falla)
                    try:
                        await save_ia_response_with_prompt(
                            db=db,
                            project_id=session.project_id,
                            provider=provider_name,
                            prompt_text=prompt_text,
                            response_text=ai_response.response_text,
                            latency_ms=int(provider_time * 1000),
                            user_question=request.final_question
                        )
                    except Exception as save_error:
                        logger.warning(f"‚ö†Ô∏è Error guardando respuesta de {provider_name}: {save_error}")
                    
                    individual_responses.append({
                        "provider": provider_name,
                        "model": model,
                        "content": ai_response.response_text,
                        "processing_time_ms": int(provider_time * 1000),
                        "success": True,
                        "timestamp": datetime.utcnow().isoformat(),
                        "prompt_used": prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                    })
                    logger.info(f"‚úÖ {provider.value.title()} respondi√≥ en {provider_time:.2f}s")
                else:
                    # Guardar error tambi√©n (sin bloquear si falla)
                    try:
                        await save_ia_response_with_prompt(
                            db=db,
                            project_id=session.project_id,
                            provider=provider_name,
                            prompt_text=prompt_text,
                            response_text="",
                            latency_ms=int(provider_time * 1000),
                            user_question=request.final_question,
                            error_message=ai_response.error_message
                        )
                    except Exception as save_error:
                        logger.warning(f"‚ö†Ô∏è Error guardando error de {provider_name}: {save_error}")
                    
                    individual_responses.append({
                        "provider": provider_name,
                        "model": model,
                        "error": ai_response.error_message or "Error desconocido",
                        "processing_time_ms": int(provider_time * 1000),
                        "success": False,
                        "timestamp": datetime.utcnow().isoformat(),
                        "prompt_used": prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                    })
                    logger.error(f"‚ùå Error en {provider.value.title()}: {ai_response.error_message}")
                
            except Exception as e:
                provider_time = time.time() - provider_start
                logger.error(f"‚ùå Error en {provider.value.title()}: {e}")
                
                # Guardar error en BD (sin bloquear si falla)
                try:
                    await save_ia_response_with_prompt(
                        db=db,
                        project_id=session.project_id,
                        provider=provider_name,
                        prompt_text=prompt_text,
                        response_text="",
                        latency_ms=int(provider_time * 1000),
                        user_question=request.final_question,
                        error_message=str(e)
                    )
                except Exception as save_error:
                    logger.warning(f"‚ö†Ô∏è Error guardando excepci√≥n de {provider_name}: {save_error}")
                
                individual_responses.append({
                    "provider": provider_name,
                    "model": "gpt-4o-mini" if provider == AIProviderEnum.OPENAI else "claude-3-haiku-20240307",
                    "error": str(e),
                    "processing_time_ms": int(provider_time * 1000),
                    "success": False,
                    "timestamp": datetime.utcnow().isoformat(),
                    "prompt_used": prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                })
        
        total_time = time.time() - start_time
        successful_responses = [r for r in individual_responses if r.get("success", False)]
        
        # Guardar las respuestas en la base de datos para uso posterior del moderador
        try:
            import json
            ai_responses_data = {
                "responses": individual_responses,
                "user_question": request.final_question,
                "timestamp": datetime.utcnow().isoformat(),
                "total_processing_time_ms": int(total_time * 1000)
            }
            
            # Actualizar la sesi√≥n con las respuestas de IAs
            session.ai_responses_json = json.dumps(ai_responses_data)
            await db.commit()
            logger.info(f"üíæ Respuestas guardadas en BD para sesi√≥n: {session_id}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error guardando respuestas en BD: {e}")
            # No fallar el endpoint por esto, solo registrar el warning
        
        logger.info(f"‚úÖ Consulta individual completada - {len(successful_responses)}/{len(individual_responses)} exitosas en {total_time:.2f}s")
        
        return {
            "session_id": session_id,
            "user_question": request.final_question,
            "individual_responses": individual_responses,
            "total_processing_time_ms": int(total_time * 1000),
            "successful_responses": len(successful_responses),
            "total_responses": len(individual_responses),
            "prompts_generated": prompts_result,
            "prompts_saved_to_db": True,
            "context_used": prompts_result.get('context_summary', '')[:200] + "..." if len(prompts_result.get('context_summary', '')) > 200 else prompts_result.get('context_summary', ''),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error consultando IAs individualmente: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error consultando IAs: {str(e)}"
        )


@router.post("/context-sessions/{session_id}/retry-ai/{provider}")
async def retry_single_ai(
    session_id: UUID,
    provider: str,
    request: ContextFinalizeRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Reintenta consultar una IA espec√≠fica que fall√≥ anteriormente.
    """
    logger.info(f"üîÑ Reintentando {provider.upper()} - Sesi√≥n: {session_id}")
    
    try:
        # Verificar que la sesi√≥n existe y pertenece al usuario
        session = await context_crud.get_context_session(db, session_id)
        if not session:
            raise HTTPException(
                status_code=404,
                detail="Sesi√≥n de contexto no encontrada"
            )

        user_id = current_user.get("user_id")
        if session.user_id != user_id:
            raise HTTPException(
                status_code=403,
                detail="No tienes acceso a esta sesi√≥n"
            )

        # Obtener el modelo de sesi√≥n para el contexto
        session_model = await context_crud.get_context_session_model(db, session_id)
        if not session_model:
            raise HTTPException(
                status_code=404,
                detail="Modelo de sesi√≥n no encontrado"
            )

        # Importar servicios necesarios
        from app.services.ai_orchestrator import AIOrchestrator
        from app.schemas.ai_response import AIRequest
        import time

        # Crear instancia del orquestrador
        orchestrator = AIOrchestrator()
        
        # Usar el contexto acumulado de la sesi√≥n
        context_text = session_model.accumulated_context or ""
        
        # Crear prompt completo
        full_prompt = f"""Contexto:
{context_text}

Pregunta del usuario:
{request.final_question}

Por favor, proporciona una respuesta detallada basada en el contexto proporcionado."""

        ai_request = AIRequest(
            prompt=full_prompt,
            max_tokens=1200,
            temperature=0.7,
            user_id=str(user_id),
            project_id=str(session.project_id)
        )

        # Mapear el proveedor string a enum
        from app.services.ai_adapters.base import AIProviderEnum
        provider_map = {
            "openai": AIProviderEnum.OPENAI,
            "anthropic": AIProviderEnum.ANTHROPIC
        }
        
        if provider.lower() not in provider_map:
            raise HTTPException(
                status_code=400,
                detail=f"Proveedor no v√°lido: {provider}. Debe ser 'openai' o 'anthropic'"
            )

        provider_enum = provider_map[provider.lower()]
        
        # Ejecutar consulta individual
        start_time = time.time()
        logger.info(f"ü§ñ Reintentando {provider.upper()}...")
        
        ai_response = await orchestrator.generate_single_response(ai_request, provider_enum)
        provider_time = time.time() - start_time
        
        # Procesar respuesta
        if ai_response.status.value == "success":
            response_data = {
                "provider": provider.lower(),
                "model": ai_response.model,
                "content": ai_response.response_text,
                "processing_time_ms": int(provider_time * 1000),
                "success": True,
                "timestamp": datetime.utcnow().isoformat()
            }
            logger.info(f"‚úÖ {provider.upper()} respondi√≥ correctamente en {provider_time:.2f}s")
        else:
            response_data = {
                "provider": provider.lower(),
                "model": ai_response.model or "unknown",
                "error": ai_response.error_message or "Error desconocido",
                "processing_time_ms": int(provider_time * 1000),
                "success": False,
                "timestamp": datetime.utcnow().isoformat()
            }
            logger.error(f"‚ùå {provider.upper()} fall√≥ nuevamente: {ai_response.error_message}")

        total_time = time.time() - start_time
        
        return {
            "session_id": str(session_id),
            "provider": provider.lower(),
            "response": response_data,
            "total_processing_time_ms": int(total_time * 1000),
            "timestamp": datetime.utcnow().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error reintentando {provider}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error interno reintentando {provider}: {str(e)}"
        )


@router.post("/context-sessions/{session_id}/generate-moderator-prompt")
async def generate_moderator_prompt(
    session_id: UUID,
    request: ContextFinalizeRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Genera y muestra el prompt que se enviar√° al moderador para la s√≠ntesis,
    sin ejecutar la s√≠ntesis real.
    """
    try:
        logger.info(f"üìù Generando prompt del moderador - Sesi√≥n: {session_id}")
        
        # Verificar sesi√≥n
        db_session = await context_crud.get_context_session(db, session_id)
        if not db_session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sesi√≥n no encontrada"
            )
        
        # Verificar que hay respuestas de IAs disponibles
        if not hasattr(db_session, 'ai_responses_json') or not db_session.ai_responses_json:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No hay respuestas de IAs disponibles para generar el prompt del moderador"
            )
        
        # Parsear las respuestas de IAs desde JSON
        import json
        ai_responses_data = json.loads(db_session.ai_responses_json)
        
        # Manejar diferentes formatos de datos (puede ser lista o diccionario)
        if isinstance(ai_responses_data, list):
            responses_list = ai_responses_data
        elif isinstance(ai_responses_data, dict):
            responses_list = ai_responses_data.get('responses', [])
        else:
            responses_list = []
        
        # Verificar que hay respuestas exitosas
        successful_responses = [
            resp for resp in responses_list
            if resp.get('success', False) and resp.get('content')
        ]
        
        if not successful_responses:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No hay respuestas exitosas de IAs para generar el prompt del moderador"
            )
        
        # Crear objetos StandardAIResponse simulados para generar el prompt
        from app.schemas.ai_response import StandardAIResponse, AIResponseStatus, AIProviderEnum
        
        # DEBUG: Agregar logging para ver los datos
        logger.info(f"üîç DEBUG - ai_responses_data type: {type(ai_responses_data)}")
        logger.info(f"üîç DEBUG - responses_list length: {len(responses_list)}")
        logger.info(f"üîç DEBUG - successful_responses length: {len(successful_responses)}")
        
        mock_responses = []
        for i, resp_data in enumerate(successful_responses):
            logger.info(f"üîç DEBUG - Response {i}: {resp_data}")
            
            provider_name = resp_data.get('provider', 'unknown').lower()
            provider_enum = AIProviderEnum.OPENAI if provider_name == 'openai' else AIProviderEnum.ANTHROPIC
            
            # DEBUG: Verificar campos espec√≠ficos
            content = resp_data.get('content', '')
            processing_time = resp_data.get('processing_time_ms', 0)
            logger.info(f"üîç DEBUG - content type: {type(content)}, processing_time type: {type(processing_time)}, value: {processing_time}")
            
            mock_response = StandardAIResponse(
                response_text=content,
                status=AIResponseStatus.SUCCESS,
                ia_provider_name=provider_enum,
                latency_ms=processing_time,
                error_message=None,
                timestamp=datetime.utcnow()
            )
            mock_responses.append(mock_response)
        
        # Crear instancia del moderador solo para generar el prompt
        from app.services.ai_moderator import AIModerator
        moderator = AIModerator()
        
        # Generar el prompt usando el m√©todo interno
        moderator_prompt = moderator._create_synthesis_prompt(mock_responses)
        
        if not moderator_prompt:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="No se pudo generar el prompt del moderador"
            )
        
        # Preparar metadatos
        response_data = {
            "session_id": str(session_id),
            "moderator_prompt": moderator_prompt,
            "prompt_length": len(moderator_prompt),
            "responses_count": len(successful_responses),
            "providers_included": [resp.get('provider', 'unknown') for resp in successful_responses],
            "generated_at": datetime.utcnow().isoformat(),
            "prompt_preview": moderator_prompt[:200] + "..." if len(moderator_prompt) > 200 else moderator_prompt
        }
        
        logger.info(f"‚úÖ Prompt del moderador generado - Longitud: {len(moderator_prompt)} caracteres")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error generando prompt del moderador: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generando prompt del moderador: {str(e)}"
        )


@router.post("/context-sessions/{session_id}/synthesize")
async def synthesize_ai_responses(
    session_id: UUID,
    request: ContextFinalizeRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Ejecuta la s√≠ntesis del moderador usando las respuestas de las IAs guardadas.
    """
    try:
        logger.info(f"üî¨ Ejecutando s√≠ntesis del moderador - Sesi√≥n: {session_id}")
        
        # Verificar sesi√≥n
        db_session = await context_crud.get_context_session(db, session_id)
        if not db_session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sesi√≥n no encontrada"
            )
        
        # Buscar respuestas de IAs en la tabla ia_responses usando el context_session_id
        from sqlalchemy import select
        from app.models.models import IAResponse, IAPrompt
        
        # Buscar prompts asociados a esta sesi√≥n
        prompts_query = select(IAPrompt).where(
            IAPrompt.context_session_id == session_id,
            IAPrompt.deleted_at.is_(None)
        )
        prompts_result = await db.execute(prompts_query)
        prompts = prompts_result.scalars().all()
        
        if not prompts:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No hay prompts asociados a esta sesi√≥n"
            )
        
        logger.info(f"üìã Encontrados {len(prompts)} prompts para la sesi√≥n {session_id}")
        
        # Buscar respuestas de IAs para estos prompts
        prompt_ids = [prompt.id for prompt in prompts]
        responses_query = select(IAResponse).where(
            IAResponse.ia_prompt_id.in_(prompt_ids),
            IAResponse.deleted_at.is_(None),
            IAResponse.error_message.is_(None)  # Solo respuestas exitosas
        )
        responses_result = await db.execute(responses_query)
        ia_responses = responses_result.scalars().all()
        
        if not ia_responses:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No hay respuestas exitosas de IAs para sintetizar"
            )
        
        logger.info(f"‚úÖ Encontradas {len(ia_responses)} respuestas exitosas de IAs")
        
        # Crear objetos StandardAIResponse para la s√≠ntesis
        from app.schemas.ai_response import StandardAIResponse, AIResponseStatus, AIProviderEnum
        
        mock_responses = []
        for ia_response in ia_responses:
            # Determinar el proveedor
            provider_name = ia_response.ia_provider_name.lower()
            if provider_name == 'openai':
                provider_enum = AIProviderEnum.OPENAI
            elif provider_name == 'anthropic':
                provider_enum = AIProviderEnum.ANTHROPIC
            else:
                provider_enum = AIProviderEnum.OPENAI  # Default
            
            mock_response = StandardAIResponse(
                response_text=ia_response.raw_response_text,
                status=AIResponseStatus.SUCCESS,
                ia_provider_name=provider_enum,
                latency_ms=ia_response.latency_ms,
                error_message=None,
                timestamp=ia_response.received_at
            )
            mock_responses.append(mock_response)
        
        logger.info(f"üîÑ Preparadas {len(mock_responses)} respuestas para s√≠ntesis")
        
        # Ejecutar s√≠ntesis del moderador
        from app.services.ai_moderator import AIModerator
        moderator = AIModerator()
        
        import time
        start_time = time.time()
        synthesis_result = await moderator.synthesize_responses(mock_responses)
        synthesis_time = time.time() - start_time
        
        if not synthesis_result or not synthesis_result.synthesis_text:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="No se pudo generar la s√≠ntesis del moderador"
            )
        
        # Preparar respuesta estructurada
        response_data = {
            "session_id": str(session_id),
            "synthesis": {
                "text": synthesis_result.synthesis_text,
                "quality": synthesis_result.quality.value if synthesis_result.quality else "unknown",
                "key_themes": synthesis_result.key_themes,
                "consensus_areas": synthesis_result.consensus_areas,
                "contradictions": synthesis_result.contradictions,
                "recommendations": synthesis_result.recommendations,
                "suggested_questions": synthesis_result.suggested_questions,
                "research_areas": synthesis_result.research_areas,
                "connections": synthesis_result.connections,
                "source_references": synthesis_result.source_references
            },
            "metadata": {
                "responses_analyzed": len(ia_responses),
                "providers_included": [resp.ia_provider_name for resp in ia_responses],
                "synthesis_time_ms": int(synthesis_time * 1000),
                "processing_time_ms": synthesis_result.processing_time_ms,
                "original_responses_count": synthesis_result.original_responses_count,
                "successful_responses_count": synthesis_result.successful_responses_count,
                "fallback_used": synthesis_result.fallback_used,
                "meta_analysis_quality": synthesis_result.meta_analysis_quality,
                "generated_at": datetime.utcnow().isoformat()
            }
        }
        
        logger.info(f"‚úÖ S√≠ntesis completada - {len(ia_responses)} respuestas analizadas en {synthesis_time:.2f}s")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error ejecutando s√≠ntesis: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error ejecutando s√≠ntesis: {str(e)}"
        )


# ================================
# NUEVOS ENDPOINTS PARA FLUJO DE PROMPTS
# ================================

@router.post("/projects/{project_id}/generate-prompt")
async def generate_prompt_for_project(
    project_id: UUID,
    request: GeneratePromptRequest,
    current_user: SessionUser = Depends(require_auth),
    db: AsyncSession = Depends(get_db)
) -> GeneratePromptResponse:
    """
    Genera un prompt para las IAs basado en la consulta del usuario.
    Este es el primer paso del flujo: generar -> editar (opcional) -> ejecutar
    """
    try:
        logger.info(f"üéØ Generando prompt para proyecto {project_id} - Query: {request.query[:100]}...")
        
        # Obtener contexto si hay session_id
        context_data = ""
        if request.context_session_id:
            db_session = await context_crud.get_context_session(db, request.context_session_id)
            if db_session:
                session_model = context_crud.convert_interaction_to_context_session(db_session)
                context_data = session_model.accumulated_context
                logger.info(f"üìã Usando contexto de sesi√≥n {request.context_session_id} - {len(context_data)} chars")
        
        # Generar prompt usando el template universal
        from app.services.prompt_templates import PromptTemplateManager
        from app.schemas.ai_response import AIProviderEnum
        
        prompt_manager = PromptTemplateManager()
        prompt_data = prompt_manager.build_prompt_for_provider(
            provider=AIProviderEnum.OPENAI,  # Usamos OpenAI como referencia (ambos usan el mismo template)
            user_question=request.query,
            context_text=context_data
        )
        
        # Combinar system y user message en un solo prompt
        generated_prompt = f"{prompt_data['system_message']}\n\n{prompt_data['user_message']}"
        
        # Guardar el prompt en la base de datos
        ia_prompt = await ia_prompt_crud.create_ia_prompt(
            db=db,
            project_id=project_id,
            context_session_id=request.context_session_id,
            original_query=request.query,
            generated_prompt=generated_prompt
        )
        
        logger.info(f"‚úÖ Prompt generado y guardado - ID: {ia_prompt.id}, Longitud: {len(generated_prompt)} chars")
        
        return GeneratePromptResponse(
            prompt_id=ia_prompt.id,
            original_query=ia_prompt.original_query,
            generated_prompt=ia_prompt.generated_prompt,
            project_id=ia_prompt.project_id,
            context_session_id=ia_prompt.context_session_id,
            status=ia_prompt.status,
            created_at=ia_prompt.created_at
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error generando prompt: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generando prompt: {str(e)}"
        )


@router.get("/prompts/{prompt_id}")
async def get_prompt_by_id(
    prompt_id: UUID,
    current_user: SessionUser = Depends(require_auth),
    db: AsyncSession = Depends(get_db)
) -> IAPromptResponse:
    """
    Obtiene un prompt espec√≠fico por su ID, incluyendo sus respuestas de IA si las tiene.
    """
    try:
        ia_prompt = await ia_prompt_crud.get_ia_prompt_by_id(db, prompt_id)
        if not ia_prompt:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Prompt no encontrado"
            )
        
        return IAPromptResponse.from_orm(ia_prompt)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo prompt {prompt_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo prompt: {str(e)}"
        )


@router.put("/prompts/{prompt_id}")
async def update_prompt(
    prompt_id: UUID,
    request: IAPromptUpdate,
    current_user: SessionUser = Depends(require_auth),
    db: AsyncSession = Depends(get_db)
) -> IAPromptResponse:
    """
    Actualiza un prompt con una versi√≥n editada por el usuario.
    """
    try:
        logger.info(f"‚úèÔ∏è Actualizando prompt {prompt_id} con versi√≥n editada")
        
        updated_prompt = await ia_prompt_crud.update_ia_prompt(
            db=db,
            prompt_id=prompt_id,
            edited_prompt=request.edited_prompt
        )
        
        if not updated_prompt:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Prompt no encontrado"
            )
        
        logger.info(f"‚úÖ Prompt {prompt_id} actualizado - Nuevo status: {updated_prompt.status}")
        
        return IAPromptResponse.from_orm(updated_prompt)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error actualizando prompt {prompt_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error actualizando prompt: {str(e)}"
        )


@router.post("/prompts/{prompt_id}/execute")
async def execute_prompt(
    prompt_id: UUID,
    request: ExecutePromptRequest,
    current_user: SessionUser = Depends(require_auth),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """
    Ejecuta un prompt (original o editado) consultando a las IAs.
    Este es el paso final del flujo: generar -> editar (opcional) -> ejecutar
    """
    try:
        logger.info(f"üöÄ Ejecutando prompt {prompt_id} - Usar versi√≥n editada: {request.use_edited_version}")
        
        # Obtener el prompt
        ia_prompt = await ia_prompt_crud.get_ia_prompt_by_id(db, prompt_id)
        if not ia_prompt:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Prompt no encontrado"
            )
        
        # Determinar qu√© prompt usar
        if request.use_edited_version and ia_prompt.edited_prompt:
            prompt_to_use = ia_prompt.edited_prompt
            logger.info(f"üìù Usando versi√≥n editada del prompt")
        else:
            prompt_to_use = ia_prompt.generated_prompt
            logger.info(f"üéØ Usando versi√≥n original del prompt")
        
        # Ejecutar consultas a las IAs
        from app.services.ai_orchestrator import AIOrchestrator
        from app.schemas.ai_response import AIProviderEnum, AIRequest
        import time
        
        # Crear instancia del orquestador
        orchestrator = AIOrchestrator()
        
        responses = {}
        providers = [AIProviderEnum.OPENAI, AIProviderEnum.ANTHROPIC]
        
        for provider in providers:
            try:
                logger.info(f"ü§ñ Consultando {provider.value}...")
                start_time = time.time()
                
                # Crear request para el proveedor
                ai_request = AIRequest(
                    prompt=prompt_to_use,
                    max_tokens=1200,
                    temperature=0.7,
                    project_id=str(ia_prompt.project_id),
                    user_id=str(current_user.id)
                )
                
                # Consultar al proveedor
                response = await orchestrator.generate_single_response(
                    ai_request, provider
                )
                
                processing_time = int((time.time() - start_time) * 1000)
                
                # Guardar respuesta en la base de datos
                ia_response = await ia_prompt_crud.create_ia_response(
                    db=db,
                    ia_prompt_id=ia_prompt.id,
                    provider=provider.value,
                    response_text=response.response_text if response.response_text else "Error en la respuesta",
                    status=response.status.value,
                    latency_ms=response.latency_ms,
                    error_message=response.error_message
                )
                
                responses[provider.value] = {
                    "id": str(ia_response.id),
                    "provider": provider.value,
                    "status": response.status.value,
                    "response_text": response.response_text,
                    "latency_ms": response.latency_ms,
                    "error_message": response.error_message,
                    "success": response.status.value == "success",
                    "content": response.response_text,
                    "processing_time_ms": processing_time
                }
                
                if response.status.value == "success":
                    logger.info(f"‚úÖ {provider.value} respondi√≥ en {processing_time}ms")
                else:
                    logger.warning(f"‚ö†Ô∏è {provider.value} fall√≥: {response.error_message}")
                
            except Exception as e:
                processing_time = int((time.time() - start_time) * 1000) if 'start_time' in locals() else 0
                logger.error(f"‚ùå Error consultando {provider.value}: {str(e)}")
                responses[provider.value] = {
                    "provider": provider.value,
                    "status": "error",
                    "error_message": f"Error interno: {str(e)}",
                    "success": False,
                    "processing_time_ms": processing_time
                }
        
        successful_count = len([r for r in responses.values() if r.get("success", False)])
        
        # Commit de todas las respuestas guardadas
        await db.commit()
        
        # Marcar prompt como ejecutado
        await ia_prompt_crud.mark_prompt_as_executed(db, prompt_id)
        
        # Preparar respuesta
        response_data = {
            "prompt_id": str(prompt_id),
            "prompt_used": prompt_to_use,
            "used_edited_version": request.use_edited_version,
            "responses": responses,
            "successful_responses": successful_count,
            "total_responses": len(responses),
            "executed_at": datetime.utcnow().isoformat()
        }
        
        logger.info(f"‚úÖ Prompt ejecutado - {successful_count}/{len(responses)} respuestas exitosas")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error ejecutando prompt {prompt_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error ejecutando prompt: {str(e)}"
        )


# Funci√≥n auxiliar para guardar respuestas de IA con sus prompts
async def save_ia_response_with_prompt(
    db: AsyncSession,
    project_id: UUID,
    provider: str,
    prompt_text: str,
    response_text: str,
    latency_ms: int,
    user_question: str,
    error_message: str = None
) -> None:
    """
    Guarda una respuesta de IA junto con su prompt en la base de datos.
    """
    try:
        from app.models.models import IAResponse
        from uuid import uuid4
        from datetime import datetime
        
        # Crear registro de respuesta IA con prompt
        ia_response = IAResponse(
            id=uuid4(),
            project_id=project_id,
            prompt_text=prompt_text,
            ia_provider_name=provider,
            raw_response_text=response_text or "",
            latency_ms=latency_ms,
            error_message=error_message,
            received_at=datetime.utcnow(),
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        db.add(ia_response)
        await db.commit()
        
        logger.info(f"üíæ Respuesta de {provider} guardada en BD con prompt (longitud: {len(prompt_text)} chars)")
        
    except Exception as e:
        logger.error(f"‚ùå Error guardando respuesta de {provider} en BD: {e}")
        await db.rollback()
        # No re-raise la excepci√≥n para no romper el flujo principal


 
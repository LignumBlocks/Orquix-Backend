import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from uuid import UUID, uuid4
from pydantic import BaseModel

import openai
from app.core.config import settings

# Definici√≥n de funciones disponibles para el LLM
CONTEXT_FUNCTIONS = [
    {
        "name": "summary",
        "description": "Resume el contexto actual de manera concisa",
        "parameters": {
            "type": "object",
            "properties": {
                "max_sentences": {
                    "type": "integer",
                    "description": "N√∫mero m√°ximo de oraciones en el resumen (por defecto 2)",
                    "default": 2,
                    "minimum": 1,
                    "maximum": 5
                }
            },
            "required": []
        }
    },
    {
        "name": "show_context",
        "description": "Muestra el contexto completo actual sin modificarlo",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "clear_context",
        "description": "Borra completamente todo el contexto acumulado",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": []
        }
    }
]

# Modelos temporales para compatibilidad
class ContextMessage(BaseModel):
    role: str
    content: str
    timestamp: datetime
    message_type: Optional[str] = None

class ContextChatResponse(BaseModel):
    session_id: UUID
    ai_response: str
    message_type: str
    accumulated_context: str
    suggestions: List[str]
    context_elements_count: int

class ContextSession(BaseModel):
    id: UUID
    project_id: UUID
    user_id: Optional[UUID]
    conversation_history: List[ContextMessage]
    accumulated_context: str
    is_active: bool
    created_at: datetime
    updated_at: datetime

logger = logging.getLogger(__name__)

# Funciones helper fuera de la clase
async def classify_message_llm(
    client: openai.AsyncOpenAI,
    user_message: str,
    model: str = "gpt-3.5-turbo"
) -> Tuple[str, float]:
    """
    Clasifica un mensaje usando LLM de forma agn√≥stica al idioma.
    
    Args:
        client: Cliente de OpenAI
        user_message: Mensaje del usuario
        model: Modelo a usar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    prompt = (
        "You are a language-agnostic classifier. "
        "Return ONLY valid JSON with keys: "
        "message_type ('question' or 'information') and confidence (0-1). "
        f"Text: ¬´{user_message.strip()}¬ª"
    )
    try:
        chat = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=50,
            response_format={"type": "json_object"}
        )
        data = json.loads(chat.choices[0].message.content)
        if data["message_type"] in ("question", "information"):
            return data["message_type"], float(data["confidence"])
    except Exception as e:
        logger.debug(f"LLM classify failed ‚Üí fallback: {e}")
    return _fallback_heuristic(user_message)

def _fallback_heuristic(msg: str) -> Tuple[str, float]:
    """
    Heur√≠stica universal de fallback para clasificaci√≥n.
    
    Args:
        msg: Mensaje a clasificar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    text = msg.strip()
    if text.endswith("?") or (text.count("?") == 1 and len(text) < 80):
        return "question", 0.6
    if len(text.split()) > 15:
        return "information", 0.6
    return "question", 0.5

class ContextBuilderService:
    """
    Servicio para construcci√≥n conversacional de contexto usando GPT-3.5.
    
    Este servicio ayuda a los usuarios a construir contexto de manera fluida
    antes de enviar consultas a las IAs principales.
    """
    
    def __init__(self):
        self.client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = "gpt-3.5-turbo"
        self.temperature = 0.7  # M√°s natural y conversacional
        self.max_tokens = 400   # Respuestas m√°s completas para chat normal
        self.seed = None        # Sin seed para m√°s naturalidad en conversaci√≥n
    
    async def _smart_classify(self, user_message: str) -> Tuple[str, float]:
        """
        Clasifica un mensaje usando LLM multiling√ºe con fallback universal.
        
        Args:
            user_message: Mensaje del usuario
            
        Returns:
            Tuple con (message_type, confidence)
        """
        return await classify_message_llm(self.client, user_message, self.model)
    
    def _execute_function(self, function_call, current_context: str) -> Tuple[str, str]:
        """
        Ejecuta una funci√≥n solicitada por el LLM.
        
        Args:
            function_call: Objeto function_call de OpenAI
            current_context: Contexto actual
            
        Returns:
            Tuple con (result_message, updated_context)
        """
        function_name = function_call.name
        
        try:
            # Parsear argumentos si existen
            arguments = {}
            if function_call.arguments:
                arguments = json.loads(function_call.arguments)
        except json.JSONDecodeError:
            arguments = {}
        
        if function_name == "summary":
            # Resume el contexto actual
            max_sentences = arguments.get("max_sentences", 2)
            return self._create_context_summary(current_context, max_sentences), current_context
            
        elif function_name == "show_context":
            # Muestra el contexto completo
            if not current_context.strip():
                return "üìã **Contexto actual**: No hay contexto acumulado a√∫n.", current_context
            else:
                word_count = len(current_context.split())
                char_count = len(current_context)
                return f"üìã **Contexto actual** ({word_count} palabras, {char_count} caracteres):\n\n{current_context}", current_context
                
        elif function_name == "clear_context":
            # Borra todo el contexto
            return "üóëÔ∏è **Contexto borrado**: He eliminado todo el contexto acumulado. Podemos empezar desde cero.", ""
            
        else:
            # Funci√≥n desconocida
            return f"‚ùå **Error**: Funci√≥n '{function_name}' no reconocida.", current_context
    
    def _create_context_summary(self, context: str, max_sentences: int = 2) -> str:
        """
        Crea un resumen conciso del contexto.
        
        Args:
            context: Contexto a resumir
            max_sentences: N√∫mero m√°ximo de oraciones
            
        Returns:
            Resumen del contexto
        """
        if not context.strip():
            return "üìã **Resumen**: No hay contexto para resumir a√∫n."
        
        # Dividir en oraciones (aproximado)
        sentences = []
        for sentence in context.replace('. ', '.\n').split('\n'):
            sentence = sentence.strip()
            if sentence and not sentence.endswith('.'):
                sentence += '.'
            if sentence and len(sentence) > 10:  # Filtrar oraciones muy cortas
                sentences.append(sentence)
        
        # Seleccionar las primeras N oraciones m√°s importantes
        if len(sentences) <= max_sentences:
            summary = ' '.join(sentences)
        else:
            # Tomar las primeras oraciones (usualmente contienen info m√°s importante)
            summary = ' '.join(sentences[:max_sentences])
        
        word_count = len(context.split())
        return f"üìã **Resumen del contexto** ({word_count} palabras totales):\n\n{summary}"
    
    async def _extract_information_from_message(
        self, 
        user_message: str, 
        current_context: str, 
        conversation_history: List[ContextMessage] = None
    ) -> str:
        """
        Extrae informaci√≥n espec√≠fica de un mensaje del usuario usando el contexto conversacional.
        
        Args:
            user_message: Mensaje del usuario
            current_context: Contexto actual para evitar duplicaci√≥n
            conversation_history: Historial reciente de la conversaci√≥n
            
        Returns:
            Informaci√≥n extra√≠da del mensaje con contexto conversacional
        """
        
        if conversation_history is None:
            conversation_history = []
        # Llamada simple a GPT para extraer informaci√≥n espec√≠fica
        try:
            # Construir historial conversacional reciente para contexto
            recent_conversation = ""
            if conversation_history:
                recent_messages = conversation_history[-4:]  # √öltimos 4 mensajes para contexto
                for i, msg in enumerate(recent_messages):
                    role_emoji = "üë§" if msg.role == "user" else "ü§ñ"
                    recent_conversation += f"{role_emoji} {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}\n"
            
            prompt = f"""
            Extrae toda la informaci√≥n relevante de este mensaje del usuario, usando el contexto de la conversaci√≥n para entender mejor las referencias:

            MENSAJE ACTUAL: "{user_message}"

            CONTEXTO CONVERSACIONAL RECIENTE:
            {recent_conversation if recent_conversation.strip() else "No hay conversaci√≥n previa"}

            CONTEXTO ACUMULADO DEL PROYECTO:
            {current_context if current_context.strip() else "No hay contexto previo"}

            INSTRUCCIONES:
            - Usa el contexto conversacional para entender referencias como "opci√≥n 2", "eso", "lo anterior", etc.
            - Extrae informaci√≥n espec√≠fica y contextual (ej: "Decisi√≥n: Opci√≥n 2 para construir startup de envases de banana")
            - Incluye informaci√≥n sobre decisiones, preferencias, n√∫meros espec√≠ficos, ubicaciones, restricciones
            - Evita repetir exactamente lo que ya est√° en el contexto acumulado
            - Si el mensaje es solo una referencia sin contexto conversacional, extrae lo que puedas
            - S√© espec√≠fico y √∫til para futuras consultas
            - Si no hay informaci√≥n nueva, responde ""

            INFORMACI√ìN EXTRA√çDA:
            """
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=200
            )
            
            extracted = response.choices[0].message.content.strip()
            return extracted if extracted and extracted != '""' else ""
            
        except Exception as e:
            logger.debug(f"Error extrayendo informaci√≥n: {e}")
            # Fallback: extraer informaci√≥n b√°sica usando heur√≠sticas
            return self._extract_info_heuristic(user_message, current_context, conversation_history)
    
    def _extract_info_heuristic(
        self, 
        user_message: str, 
        current_context: str,
        conversation_history: List[ContextMessage] = None
    ) -> str:
        """
        Extrae informaci√≥n usando heur√≠sticas simples como fallback.
        
        Args:
            user_message: Mensaje del usuario
            current_context: Contexto actual
            conversation_history: Historial de conversaci√≥n
            
        Returns:
            Informaci√≥n extra√≠da usando heur√≠sticas
        """
        
        if conversation_history is None:
            conversation_history = []
        info_parts = []
        message_lower = user_message.lower()
        
        # Detectar referencias contextuales
        if any(ref in message_lower for ref in ['opci√≥n', 'option', 'me quedo', 'elijo', 'prefiero']):
            # Buscar en el historial qu√© opciones se mencionaron
            for msg in reversed(conversation_history[-6:]):  # √öltimos 6 mensajes
                if msg.role == "assistant" and any(word in msg.content.lower() for word in ['opci√≥n', 'option', '1.', '2.', '3.']):
                    # Intentar extraer el contexto del proyecto del historial
                    project_context = ""
                    for hist_msg in conversation_history:
                        if any(term in hist_msg.content.lower() for term in ['startup', 'empresa', 'negocio', 'envases', 'banana']):
                            project_context = hist_msg.content[:50] + "..."
                            break
                    
                    if "opci√≥n 2" in message_lower or "option 2" in message_lower:
                        info_parts.append(f"Decisi√≥n: Opci√≥n 2 seleccionada {f'(relacionado con {project_context})' if project_context else ''}")
                    elif "opci√≥n" in message_lower or "option" in message_lower:
                        info_parts.append(f"Decisi√≥n de opci√≥n tomada {f'(relacionado con {project_context})' if project_context else ''}")
                    break
        
        # Buscar n√∫meros importantes (presupuestos, objetivos, etc.)
        import re
        numbers = re.findall(r'\b\d+[,.]?\d*\b', user_message)
        if numbers:
            info_parts.append(f"N√∫meros mencionados: {', '.join(numbers)}")
        
        # Buscar palabras clave de informaci√≥n
        keywords = ['startup', 'empresa', 'negocio', 'producto', 'servicio', 'cliente', 'usuario', 'mercado', 'presupuesto']
        for keyword in keywords:
            if keyword in message_lower and keyword not in current_context.lower():
                # Extraer contexto alrededor de la palabra clave
                words = user_message.split()
                for i, word in enumerate(words):
                    if keyword in word.lower():
                        start = max(0, i-2)
                        end = min(len(words), i+3)
                        context_snippet = ' '.join(words[start:end])
                        info_parts.append(context_snippet)
                        break
        
        return '. '.join(info_parts) if info_parts else ""
    
    def _generate_contextual_suggestions(self, message_type: str, context: str) -> List[str]:
        """
        Genera sugerencias contextuales basadas en el tipo de mensaje y contexto.
        
        Args:
            message_type: Tipo de mensaje (question/information)
            context: Contexto actual
            
        Returns:
            Lista de sugerencias relevantes
        """
        if message_type == "question":
            return [
                "Comparte m√°s detalles sobre tu situaci√≥n",
                "Describe tu proyecto o empresa",
                "Menciona tus objetivos espec√≠ficos"
            ]
        else:  # information
            context_length = len(context.split()) if context.strip() else 0
            
            if context_length < 20:
                return [
                    "Agrega m√°s informaci√≥n sobre tu proyecto",
                    "Describe tus principales desaf√≠os", 
                    "Comparte tus objetivos y restricciones"
                ]
            elif context_length < 50:
                return [
                    "¬øHay algo m√°s relevante que agregar?",
                    "Describe tu situaci√≥n actual",
                    "Menciona cualquier restricci√≥n importante"
                ]
            else:
                return [
                    "El contexto est√° bastante completo",
                    "Puedes hacer preguntas espec√≠ficas ahora",
                    "¬øNecesitas resumir lo que hemos discutido?"
                ]
    
    async def process_user_message(
        self,
        user_message: str,
        conversation_history: List[ContextMessage],
        current_context: str
    ) -> ContextChatResponse:
        """
        Procesa un mensaje del usuario usando function calling.
        
        Args:
            user_message: Mensaje del usuario
            conversation_history: Historial de la conversaci√≥n
            current_context: Contexto acumulado hasta ahora
            
        Returns:
            ContextChatResponse con la respuesta y metadatos
        """
        try:
            # Clasificar el mensaje con el nuevo m√©todo
            message_type, confidence = await self._smart_classify(user_message)
            
            # Si la confianza es baja, pedir aclaraci√≥n
            if confidence < 0.55:
                return ContextChatResponse(
                    session_id=uuid4(),
                    ai_response="No estoy seguro de si eso es una pregunta o informaci√≥n. ¬øPodr√≠as aclararlo o aportar m√°s detalles?",
                    message_type="question",
                    accumulated_context=current_context,
                    suggestions=["Reformula tu mensaje", "S√© m√°s espec√≠fico", "Agrega m√°s contexto"],
                    context_elements_count=self._count_context_elements(current_context)
                )
            
            # Construir mensajes para GPT-3.5
            system_prompt = self._build_system_prompt()
            messages = self._build_conversation_messages(
                system_prompt, user_message, conversation_history, current_context
            )
            
            # Llamar a GPT-3.5 con function calling
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                functions=CONTEXT_FUNCTIONS,
                function_call="auto",
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                seed=self.seed
            )
            
            # Procesar respuesta
            choice = response.choices[0]
            updated_context = current_context
            
            # Verificar si hay function_call
            if choice.message.function_call:
                # Ejecutar funci√≥n solicitada
                function_result, updated_context = self._execute_function(choice.message.function_call, current_context)
                
                return ContextChatResponse(
                    session_id=uuid4(),
                    ai_response=function_result,
                    message_type="command_result",
                    accumulated_context=updated_context,
                    suggestions=["Contin√∫a compartiendo informaci√≥n", "Haz m√°s preguntas"],
                    context_elements_count=self._count_context_elements(updated_context)
                )
            else:
                # Respuesta normal sin function call
                ai_response = choice.message.content or "¬øPodr√≠as darme m√°s detalles?"
                
                # El contexto no se actualiza en chat diario - solo se mantiene el historial conversacional
                updated_context = current_context
                
                # Generar sugerencias contextuales (funci√≥n existente)
                suggestions = self._generate_contextual_suggestions(message_type, updated_context)
                
                return ContextChatResponse(
                    session_id=uuid4(),
                    ai_response=ai_response,
                    message_type=message_type,
                    accumulated_context=updated_context,
                    suggestions=suggestions,
                    context_elements_count=self._count_context_elements(updated_context)
                )
            
        except openai.APIError as e:
            logger.error(f"Error de API de OpenAI: {e}")
            return await self._create_fallback_response(user_message, current_context, conversation_history)
        except openai.RateLimitError as e:
            logger.error(f"Rate limit excedido en OpenAI: {e}")
            return await self._create_fallback_response(user_message, current_context, conversation_history)
        except Exception as e:
            logger.error(f"Error inesperado en context builder: {e}")
            return await self._create_fallback_response(user_message, current_context, conversation_history)
    
    def _build_system_prompt(self) -> str:
        """Construye el prompt del sistema para un chat conversacional completo."""
        return """Eres un asistente conversacional inteligente y completo que puede:

1. **RESPONDER CUALQUIER PREGUNTA** usando tu conocimiento general:
   - Preguntas sobre cualquier tema: "¬øcu√°les son los pa√≠ses m√°s exportadores de banana?"
   - Explicaciones de conceptos: "¬øqu√© es machine learning?"
   - Consejos generales: "¬øc√≥mo mejorar mi productividad?"
   - C√°lculos y an√°lisis: "¬øcu√°nto es 15% de 850?"

2. **MANTENER CONVERSACIONES NATURALES** sobre cualquier tema de inter√©s del usuario

3. **RECONOCER INFORMACI√ìN RELEVANTE** cuando el usuario comparte detalles de su proyecto/negocio:
   - N√∫meros espec√≠ficos, objetivos, restricciones
   - Datos sobre su empresa, producto, o situaci√≥n
   - Informaci√≥n que podr√≠a ser √∫til para consultas especializadas

4. **CONSTRUIR CONTEXTO GRADUALMENTE** para cuando el usuario quiera consultar IAs especializadas

**COMPORTAMIENTO:**
- Responde de manera natural y √∫til a CUALQUIER pregunta
- Si detectas informaci√≥n relevante del proyecto del usuario, recon√≥cela sutilmente
- Mant√©n un tono amigable, profesional y conversacional
- No te limites solo a "construcci√≥n de contexto" - eres un chat completo

**FUNCIONES DISPONIBLES:**
- summary(): Resume el contexto actual del proyecto
- show_context(): Muestra contexto completo del proyecto  
- clear_context(): Borra contexto del proyecto

Act√∫a como un asistente conversacional normal que ADEM√ÅS tiene la capacidad especial de construir contexto cuando es relevante."""
    
    def _build_conversation_messages(
        self,
        system_prompt: str,
        user_message: str,
        conversation_history: List[ContextMessage],
        current_context: str
    ) -> List[Dict[str, str]]:
        """Construye los mensajes para enviar a GPT-3.5."""
        messages = [{"role": "system", "content": system_prompt}]
        
        # Agregar contexto actual si existe
        if current_context.strip():
            context_info = f"""
            CONTEXTO ACTUAL DEL PROYECTO:
            {current_context}

            ---
            """
            messages.append({"role": "system", "content": context_info})
        
        # Agregar historial de conversaci√≥n (√∫ltimos 6 mensajes para no exceder l√≠mites)
        recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
        logger.info(f"üîç Context Builder recibe {len(conversation_history)} mensajes totales, usando √∫ltimos {len(recent_history)}")
        
        for i, msg in enumerate(recent_history):
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
            logger.info(f"  üìã Msg {i+1}: {msg.role} -> {msg.content[:50]}...")
        
        # Agregar mensaje actual del usuario
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def _update_accumulated_context(self, current_context: str, new_info: str) -> str:
        """
        Actualiza el contexto acumulado con nueva informaci√≥n.
        Evita duplicaci√≥n pero preserva informaci√≥n importante.
        """
        if not new_info or not new_info.strip():
            return current_context
            
        if not current_context or not current_context.strip():
            return new_info.strip()
        
        # Normalizar textos para comparaci√≥n
        current_lower = current_context.lower().strip()
        new_lower = new_info.lower().strip()
        
        # Solo evitar duplicaci√≥n exacta
        if new_lower == current_lower:
            logger.debug(f"Informaci√≥n exactamente duplicada detectada")
            return current_context
        
        # Si la nueva informaci√≥n est√° completamente contenida (pero no es exacta), agregar igual
        # para que se mantenga la informaci√≥n completa
        if new_lower in current_lower and len(new_lower) < len(current_lower) * 0.8:
            logger.debug(f"Informaci√≥n ya contenida en contexto existente")
            return current_context
        
        # Si la nueva informaci√≥n es mucho m√°s completa que la actual, reemplazar
        if len(new_info) > len(current_context) * 1.5:
            logger.debug(f"Reemplazando contexto con versi√≥n mucho m√°s completa")
            return new_info.strip()
        
        # En todos los dem√°s casos, combinar la informaci√≥n
        # Usar punto para separar informaci√≥n distinta
        if current_context.endswith('.'):
            return f"{current_context} {new_info.strip()}"
        else:
            return f"{current_context}. {new_info.strip()}"
    
    def include_moderator_synthesis(self, current_context: str, synthesis_text: str, key_themes: list = None, recommendations: list = None) -> str:
        """
        Incluye la s√≠ntesis del moderador en el contexto acumulado.
        
        Args:
            current_context: Contexto actual acumulado
            synthesis_text: Texto de s√≠ntesis del moderador
            key_themes: Temas clave identificados por el moderador (opcional)
            recommendations: Recomendaciones del moderador (opcional)
            
        Returns:
            Contexto actualizado con la s√≠ntesis del moderador
        """
        if not synthesis_text.strip():
            return current_context
        
        # Evitar duplicaci√≥n - verificar si ya est√° incluida
        if "üî¨ An√°lisis del Moderador IA" in current_context:
            logger.debug("S√≠ntesis del moderador ya incluida en el contexto")
            return current_context
        
        # Crear resumen estructurado de la s√≠ntesis
        moderator_section = "## üî¨ An√°lisis del Moderador IA\n\n"
        
        # Incluir temas clave si est√°n disponibles
        if key_themes:
            moderator_section += "**Temas Clave Identificados:**\n"
            for theme in key_themes[:3]:  # Limitar a 3 temas principales
                moderator_section += f"‚Ä¢ {theme}\n"
            moderator_section += "\n"
        
        # Incluir recomendaciones si est√°n disponibles
        if recommendations:
            moderator_section += "**Recomendaciones Principales:**\n"
            for rec in recommendations[:3]:  # Limitar a 3 recomendaciones principales
                moderator_section += f"‚Ä¢ {rec}\n"
            moderator_section += "\n"
        
        # Incluir s√≠ntesis (limitada para no abrumar el contexto)
        synthesis_preview = synthesis_text[:800] + "..." if len(synthesis_text) > 800 else synthesis_text
        moderator_section += f"**S√≠ntesis del An√°lisis:**\n{synthesis_preview}"
        
        # Combinar con el contexto existente
        if current_context.strip():
            return f"{current_context}\n\n{moderator_section}"
        else:
            return moderator_section
    
    def _count_context_elements(self, context: str) -> int:
        """Cuenta los elementos de contexto separados por l√≠neas."""
        if not context.strip():
            return 0
        
        # Contar p√°rrafos no vac√≠os
        elements = [line.strip() for line in context.split('\n') if line.strip()]
        return len(elements)
    
    async def package_context_for_orchestration(
        self,
        target_query: str,
        conversation_history: List[ContextMessage]
    ) -> str:
        """
        Funci√≥n clave: Refina el historial conversacional completo en un contexto 
        de alta calidad espec√≠ficamente dise√±ado para el panel de IAs expertas.
        
        Esta es la "magia" de nuestra nueva arquitectura.
        
        Args:
            target_query: La pregunta espec√≠fica que el usuario quiere que respondan las IAs
            conversation_history: Historial completo de la conversaci√≥n
            
        Returns:
            Contexto refinado y estructurado para orquestaci√≥n
        """
        
        if not conversation_history:
            return f"**Consulta:** {target_query}\n\n**Contexto:** Sin informaci√≥n previa disponible."
        
        try:
            # Construir el historial conversacional para el prompt
            conversation_text = ""
            for i, msg in enumerate(conversation_history, 1):
                role_indicator = "üë§ Usuario" if msg.role == "user" else "ü§ñ Asistente"
                conversation_text += f"{i}. {role_indicator}: {msg.content}\n\n"
            
            # Prompt especializado de "Ingeniero de Contexto"
            system_prompt = """**Rol:** Eres un "Ingeniero de Contexto". Tu √∫nica tarea es leer un historial de conversaci√≥n y una pregunta final, y luego preparar un brief conciso y perfectamente relevante para un panel de IAs expertas.

**Tu Tarea (Sigue estos pasos rigurosamente):**
1. Lee la `target_query` para entender el objetivo preciso del an√°lisis que se va a realizar.
2. Revisa el `conversation_history` completo.
3. **Extrae y sintetiza √öNICAMENTE la informaci√≥n** (hechos, datos, decisiones previas, restricciones, objetivos mencionados) que sea **directamente relevante** para que las IAs puedan responder a la `target_query`.
4. **Ignora todo el "ruido" conversacional:** saludos, agradecimientos, mensajes de "ok", correcciones de tipeo, preguntas ya respondidas en el hilo, y cualquier otra parte de la conversaci√≥n que no aporte valor factual para la consulta final.
5. **Estructura tu salida** como un "Brief de Consulta" limpio y bien organizado en Markdown.

**Criterios de Relevancia Espec√≠ficos:**
- Datos num√©ricos (presupuestos, objetivos, m√©tricas)
- Decisiones tomadas ("eleg√≠ opci√≥n X", "me quedo con Y")
- Restricciones y limitaciones mencionadas
- Contexto geogr√°fico y temporal
- Informaci√≥n del negocio/industria/proyecto
- Objetivos y metas espec√≠ficas

**Ejemplos de Ruido a Filtrar:**
- Saludos: "Hola", "gracias", "perfecto"
- Correcciones: "quise decir X en lugar de Y"
- Confirmaciones vac√≠as: "ok", "entendido", "genial"
- Preguntas ya respondidas en el mismo hilo
- Mensajes de cortes√≠a sin informaci√≥n factual

**Estructura del Output:**
```markdown
## üéØ Consulta Principal
[La pregunta espec√≠fica que se va a analizar]

## üìã Contexto Relevante
### Informaci√≥n del Proyecto/Negocio
[Datos clave del proyecto, empresa, producto]

### Decisiones y Preferencias
[Decisiones tomadas, opciones elegidas]

### Restricciones y Limitaciones
[Limitaciones geogr√°ficas, presupuestarias, temporales]

### Objetivos y Metas
[Objetivos espec√≠ficos mencionados]

### Datos Num√©ricos Relevantes
[Presupuestos, m√©tricas, n√∫meros importantes]
```

**Output:** Un √∫nico documento de texto estructurado que ser√° enviado a las IAs expertas."""

            user_prompt = f"""**Target Query:**
{target_query}

**Historial de Conversaci√≥n Completo:**
{conversation_text}

Por favor, genera el Brief de Consulta siguiendo exactamente la estructura especificada:"""

            # Llamada a GPT-4o-mini para m√°xima calidad en este paso cr√≠tico
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",  # Modelo m√°s potente para este paso cr√≠tico
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,  # Muy bajo para m√°xima consistencia y precisi√≥n
                max_tokens=1500   # Suficiente para contexto estructurado completo
            )
            
            refined_context = response.choices[0].message.content.strip()
            
            logger.info(f"üéØ Contexto refinado para orquestaci√≥n generado - Query: {target_query[:50]}...")
            logger.info(f"üìä Historial procesado: {len(conversation_history)} mensajes ‚Üí Contexto refinado: {len(refined_context)} caracteres")
            
            return refined_context
            
        except Exception as e:
            logger.error(f"Error en package_context_for_orchestration: {e}")
            # Fallback: crear contexto b√°sico manualmente
            return self._create_fallback_orchestration_context(target_query, conversation_history)
    
    def _create_fallback_orchestration_context(
        self,
        target_query: str,
        conversation_history: List[ContextMessage]
    ) -> str:
        """Fallback para crear contexto de orquestaci√≥n en caso de error con GPT."""
        
        # Extraer informaci√≥n clave usando heur√≠sticas
        key_info = []
        decisions = []
        numbers = []
        
        import re
        
        for msg in conversation_history:
            if msg.role == "user":
                content_lower = msg.content.lower()
                
                # Detectar decisiones
                if any(word in content_lower for word in ['elijo', 'me quedo', 'prefiero', 'opci√≥n']):
                    decisions.append(msg.content[:100])
                
                # Detectar n√∫meros importantes
                found_numbers = re.findall(r'\b\d+[,.]?\d*\b', msg.content)
                if found_numbers:
                    numbers.extend(found_numbers)
                
                # Detectar informaci√≥n de negocio
                if any(word in content_lower for word in ['startup', 'empresa', 'negocio', 'producto']):
                    key_info.append(msg.content[:150])
        
        # Construir contexto b√°sico
        context_parts = [f"## üéØ Consulta Principal\n{target_query}\n"]
        
        if key_info:
            context_parts.append("## üìã Informaci√≥n del Proyecto")
            for info in key_info[:3]:  # Top 3
                context_parts.append(f"- {info}")
            context_parts.append("")
        
        if decisions:
            context_parts.append("## ‚úÖ Decisiones Tomadas")
            for decision in decisions[:3]:  # Top 3
                context_parts.append(f"- {decision}")
            context_parts.append("")
        
        if numbers:
            context_parts.append("## üìä Datos Num√©ricos")
            context_parts.append(f"- N√∫meros mencionados: {', '.join(set(numbers[:5]))}")
        
        return "\n".join(context_parts)
    
    async def _create_fallback_response(
        self, 
        user_message: str, 
        current_context: str,
        conversation_history: List[ContextMessage] = None
    ) -> ContextChatResponse:
        """Crea una respuesta de fallback en caso de error con OpenAI."""
        
        if conversation_history is None:
            conversation_history = []
        
        # Usar el nuevo clasificador inteligente
        message_type, confidence = await self._smart_classify(user_message)
        is_question = message_type == "question"
        
        # Si la confianza es baja, pedir aclaraci√≥n
        if confidence < 0.55:
            return ContextChatResponse(
                session_id=uuid4(),
                ai_response="No estoy seguro de si eso es una pregunta o informaci√≥n. ¬øPodr√≠as aclararlo o aportar m√°s detalles?",
                message_type="question",
                accumulated_context=current_context,
                suggestions=["Reformula tu mensaje", "S√© m√°s espec√≠fico", "Agrega m√°s contexto"],
                context_elements_count=self._count_context_elements(current_context)
            )
        
        if is_question:
            # Es una pregunta - usar historial para generar respuesta contextual
            updated_context = current_context
            
            if len(conversation_history) == 0:
                # Primera interacci√≥n
                response_text = "¬°Hola! Estoy aqu√≠ para ayudarte a construir el contexto de tu consulta. ¬øPodr√≠as contarme m√°s sobre tu proyecto o situaci√≥n?"
                suggestions = [
                    "Describe tu proyecto o empresa",
                    "Comparte los principales desaf√≠os",
                    "Explica qu√© tipo de ayuda necesitas"
                ]
            else:
                # Hay historial previo - intentar dar respuesta contextual b√°sica
                logger.info(f"üîÑ Fallback con historial: {len(conversation_history)} mensajes")
                
                # Generar respuesta b√°sica basada en el contexto
                if "capital" in user_message.lower() and len(conversation_history) >= 2:
                    # Pregunta sobre capital despu√©s de hablar de pa√≠ses
                    last_assistant_msg = None
                    for msg in reversed(conversation_history):
                        if msg.role == "assistant":
                            last_assistant_msg = msg.content
                            break
                    
                    if last_assistant_msg and ("ecuador" in last_assistant_msg.lower() or "banana" in last_assistant_msg.lower()):
                        response_text = "La capital de Ecuador es Quito. ¬øTe gustar√≠a saber algo m√°s sobre Ecuador o tienes alguna otra pregunta?"
                    else:
                        response_text = "Perd√≥n, necesito m√°s contexto. ¬øDe qu√© pa√≠s te refieres la capital?"
                else:
                    # Respuesta gen√©rica pero conversacional
                    response_text = "Perd√≥n, tuve un problema t√©cnico. ¬øPodr√≠as reformular tu pregunta o darme m√°s detalles?"
                
                suggestions = [
                    "Reformula tu pregunta",
                    "Agrega m√°s contexto",
                    "Haz una pregunta m√°s espec√≠fica"
                ]
            
            message_type = "question"
            context_elements = self._count_context_elements(updated_context)
        else:
            # Es informaci√≥n
            updated_context = self._update_accumulated_context(current_context, user_message)
            response_text = "Perfecto, he registrado esa informaci√≥n. ¬øHay algo m√°s que quieras agregar sobre tu proyecto?"
            suggestions = [
                "Comparte m√°s detalles sobre tu negocio",
                "Describe tus principales desaf√≠os",
                "H√°blame de tus objetivos"
            ]
            message_type = "information"
            context_elements = self._count_context_elements(updated_context)
        
        # El chat es infinito - no hay finalizaci√≥n autom√°tica
        suggested_question = None
        
        return ContextChatResponse(
            session_id=uuid4(),
            ai_response=response_text,
            message_type=message_type,
            accumulated_context=updated_context,
            suggestions=suggestions,
            context_elements_count=context_elements
        )
    

    

    

    


# Instancia global del servicio
context_builder_service = ContextBuilderService() 
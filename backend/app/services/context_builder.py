import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from uuid import UUID, uuid4

import openai
from app.core.config import settings
from app.models.context_session import (
    ContextMessage, 
    ContextChatResponse,
    ContextSession
)

logger = logging.getLogger(__name__)

# Funciones helper fuera de la clase
async def classify_message_llm(
    client: openai.AsyncOpenAI,
    user_message: str,
    model: str = "gpt-3.5-turbo"
) -> Tuple[str, float]:
    """
    Clasifica un mensaje usando LLM de forma agn√≥stica al idioma.
    
    Args:
        client: Cliente de OpenAI
        user_message: Mensaje del usuario
        model: Modelo a usar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    prompt = (
        "You are a language-agnostic classifier. "
        "Return ONLY valid JSON with keys: "
        "message_type ('question' or 'information') and confidence (0-1). "
        f"Text: ¬´{user_message.strip()}¬ª"
    )
    try:
        chat = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=50,
            response_format={"type": "json_object"}
        )
        data = json.loads(chat.choices[0].message.content)
        if data["message_type"] in ("question", "information"):
            return data["message_type"], float(data["confidence"])
    except Exception as e:
        logger.debug(f"LLM classify failed ‚Üí fallback: {e}")
    return _fallback_heuristic(user_message)

def _fallback_heuristic(msg: str) -> Tuple[str, float]:
    """
    Heur√≠stica universal de fallback para clasificaci√≥n.
    
    Args:
        msg: Mensaje a clasificar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    text = msg.strip()
    if text.endswith("?") or (text.count("?") == 1 and len(text) < 80):
        return "question", 0.6
    if len(text.split()) > 15:
        return "information", 0.6
    return "question", 0.5

# Funciones helper fuera de la clase
async def classify_message_llm(
    client: openai.AsyncOpenAI,
    user_message: str,
    model: str = "gpt-3.5-turbo"
) -> Tuple[str, float]:
    """
    Clasifica un mensaje usando LLM de forma agn√≥stica al idioma.
    
    Args:
        client: Cliente de OpenAI
        user_message: Mensaje del usuario
        model: Modelo a usar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    prompt = (
        "You are a language-agnostic classifier. "
        "Return ONLY valid JSON with keys: "
        "message_type ('question' or 'information') and confidence (0-1). "
        f"Text: ¬´{user_message.strip()}¬ª"
    )
    try:
        chat = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=50,
            response_format={"type": "json_object"}
        )
        data = json.loads(chat.choices[0].message.content)
        if data["message_type"] in ("question", "information"):
            return data["message_type"], float(data["confidence"])
    except Exception as e:
        logger.debug(f"LLM classify failed ‚Üí fallback: {e}")
    return _fallback_heuristic(user_message)

def _fallback_heuristic(msg: str) -> Tuple[str, float]:
    """
    Heur√≠stica universal de fallback para clasificaci√≥n.
    
    Args:
        msg: Mensaje a clasificar
        
    Returns:
        Tuple con (message_type, confidence)
    """
    text = msg.strip()
    if text.endswith("?") or (text.count("?") == 1 and len(text) < 80):
        return "question", 0.6
    if len(text.split()) > 15:
        return "information", 0.6
    return "question", 0.5

class ContextBuilderService:
    """
    Servicio para construcci√≥n conversacional de contexto usando GPT-3.5.
    
    Este servicio ayuda a los usuarios a construir contexto de manera fluida
    antes de enviar consultas a las IAs principales.
    """
    
    def __init__(self):
        self.client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = "gpt-3.5-turbo"
        self.temperature = 0.3  # M√°s determin√≠stico para consistencia
        self.max_tokens = 500   # Respuestas concisas
    
    async def _smart_classify(self, user_message: str) -> Tuple[str, float]:
        """
        Clasifica un mensaje usando LLM multiling√ºe con fallback universal.
        
        Args:
            user_message: Mensaje del usuario
            
        Returns:
            Tuple con (message_type, confidence)
        """
        return await classify_message_llm(self.client, user_message, self.model)
    
    async def _smart_classify(self, user_message: str) -> Tuple[str, float]:
        """
        Clasifica un mensaje usando LLM multiling√ºe con fallback universal.
        
        Args:
            user_message: Mensaje del usuario
            
        Returns:
            Tuple con (message_type, confidence)
        """
        return await classify_message_llm(self.client, user_message, self.model)
    
    async def process_user_message(
        self,
        user_message: str,
        conversation_history: List[ContextMessage],
        current_context: str
    ) -> ContextChatResponse:
        """
        Procesa un mensaje del usuario y genera una respuesta apropiada.
        
        Args:
            user_message: Mensaje del usuario
            conversation_history: Historial de la conversaci√≥n
            current_context: Contexto acumulado hasta ahora
            
        Returns:
            ContextChatResponse con la respuesta y metadatos
        """
        try:
            # Clasificar el mensaje con el nuevo m√©todo
            message_type, confidence = await self._smart_classify(user_message)
            is_question = message_type == "question"
            
            # Si la confianza es baja, pedir aclaraci√≥n
            if confidence < 0.55:
                return ContextChatResponse(
                    session_id=uuid4(),
                    ai_response="No estoy seguro de si eso es una pregunta o informaci√≥n. ¬øPodr√≠as aclararlo o aportar m√°s detalles?",
                    message_type="question",
                    accumulated_context=current_context,
                    suggestions=["Reformula tu mensaje", "S√© m√°s espec√≠fico", "Agrega m√°s contexto"],
                    context_elements_count=self._count_context_elements(current_context),
                    suggested_final_question=None
                )
            
            # Construir prompt para GPT-3.5
            system_prompt = self._build_system_prompt()
            messages = self._build_conversation_messages(
                system_prompt, user_message, conversation_history, current_context
            )
            
            # Llamar a GPT-3.5
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"}
            )
            
            # Parsear respuesta JSON
            response_content = response.choices[0].message.content
            parsed_response = json.loads(response_content)
            
            # Extraer informaci√≥n
            message_type = parsed_response.get("message_type", "question")
            ai_response = parsed_response.get("response_text", "")
            context_update = parsed_response.get("context_update", "")
            suggestions = parsed_response.get("suggestions", [])
            suggested_final_question = parsed_response.get("suggested_final_question", "")
            
            # Actualizar contexto acumulado si hay nueva informaci√≥n
            updated_context = current_context
            if context_update and context_update.strip():
                updated_context = self._update_accumulated_context(
                    current_context, context_update
                )
            
            # Evaluar si el contexto est√° listo para finalizar
            if message_type != "ready" and self._should_suggest_finalization(updated_context):
                message_type = "ready"
                if not suggested_final_question:
                    suggested_final_question = self._generate_suggested_question(updated_context, user_message)
                ai_response = f"{ai_response}\n\nüéØ **Contexto completado!** Te sugiero esta pregunta para las IAs principales: \"{suggested_final_question}\""
                suggestions = ["Usa la pregunta sugerida", "Modifica la pregunta si lo necesitas", "Agrega m√°s contexto si falta algo"]
            
            # Contar elementos de contexto
            context_elements = self._count_context_elements(updated_context)
            
            return ContextChatResponse(
                session_id=uuid4(),  # Se actualizar√° con el ID real en el endpoint
                ai_response=ai_response,
                message_type=message_type,
                accumulated_context=updated_context,
                suggestions=suggestions,
                context_elements_count=context_elements,
                suggested_final_question=suggested_final_question if message_type == "ready" else None
            )
            
        except openai.APIError as e:
            logger.error(f"Error de API de OpenAI: {e}")
            return await self._create_fallback_response(user_message, current_context)
        except openai.RateLimitError as e:
            logger.error(f"Rate limit excedido en OpenAI: {e}")
            return await self._create_fallback_response(user_message, current_context)
        except json.JSONDecodeError as e:
            logger.warning(f"Error parsing GPT-3.5 response, reintentando con fallback: {e}")
            # Intentar una segunda vez con un prompt m√°s simple
            try:
                simple_response = await self._simple_gpt_call(user_message, current_context)
                return simple_response
            except Exception:
                return await self._create_fallback_response(user_message, current_context)
        except Exception as e:
            logger.error(f"Error inesperado en context builder: {e}")
            return await self._create_fallback_response(user_message, current_context)
    
    def _build_system_prompt(self) -> str:
        """Construye el prompt del sistema para GPT-3.5."""
        return """
Eres un asistente especializado en ayudar a usuarios a construir contexto para consultas complejas.

Tu rol es:
1. IDENTIFICAR si el usuario est√°:
   - PREGUNTANDO algo (necesita informaci√≥n/clarificaci√≥n)
   - APORTANDO informaci√≥n (agregando al contexto del proyecto)

2. EXTRAER informaci√≥n √∫til de CUALQUIER mensaje (tanto preguntas como declaraciones)

3. PRESERVAR la informaci√≥n completa y espec√≠fica del mensaje del usuario

4. GUIAR al usuario para obtener informaci√≥n completa y √∫til

5. SUGERIR FINALIZACI√ìN cuando el contexto est√© completo con una pregunta espec√≠fica

INSTRUCCIONES CR√çTICAS PARA context_update:
- SIEMPRE extrae informaci√≥n √∫til del mensaje, incluso si es una pregunta
- Las PREGUNTAS pueden contener informaci√≥n valiosa: objetivos, restricciones, mercados, presupuestos, etc.
- En "context_update" incluye TODA la informaci√≥n relevante del mensaje actual
- PRESERVA los detalles espec√≠ficos: n√∫meros, fechas, ubicaciones, objetivos, restricciones
- NO resumas excesivamente - mant√©n la informaci√≥n completa y descriptiva
- NUNCA repitas informaci√≥n que ya est√° en el contexto existente
- SOLO agrega informaci√≥n NUEVA y DIFERENTE
- S√© espec√≠fico y detallado: incluye todos los hechos importantes del usuario
- Si no hay informaci√≥n nueva, deja "context_update" vac√≠o

REGLAS ESTRICTAS PARA EVITAR DUPLICACI√ìN:
- Si el contexto ya menciona "startup de software dental", NO vuelvas a mencionarlo
- Si el contexto ya dice "fase beta", NO lo repitas
- SOLO agrega los elementos NUEVOS del mensaje actual
- Usa frases concisas y directas sin redundancia
- Evita repetir el tipo de empresa, industria o estado si ya est√°n en el contexto

EJEMPLOS DE EXTRACCI√ìN SIN DUPLICACI√ìN:

CORRECTO - Contexto existente + mensaje nuevo:
Contexto: "Startup que ofrece software de gesti√≥n para cl√≠nicas dentales, actualmente en fase beta"
Usuario: "Estamos considerando campa√±as en Google Ads pero no sabemos cu√°nto invertir"
context_update: "Considerando campa√±as en Google Ads. Incertidumbre sobre cantidad de inversi√≥n publicitaria."

INCORRECTO - Con duplicaci√≥n:
context_update: "Considerando campa√±as en Google Ads para la startup de software de gesti√≥n para cl√≠nicas dentales en fase beta"

CORRECTO - Pregunta con informaci√≥n valiosa:
Usuario: "¬øQu√© estrategia de marketing me permite alcanzar 50 clientes en M√©xico y Colombia con un CAC ‚â§ 150 USD y presupuesto de 2,000 USD/mes?"
message_type: "question"
context_update: "Objetivo: 50 clientes de pago. Mercados: M√©xico y Colombia. CAC m√°ximo: 150 USD. Presupuesto mensual: 2,000 USD. Enfoque: estrategia de marketing"

CORRECTO - Declaraci√≥n informativa:
Usuario: "Tengo una startup que ofrece software de gesti√≥n para cl√≠nicas dentales. A√∫n estamos en fase beta"
message_type: "information"
context_update: "Startup que ofrece software de gesti√≥n para cl√≠nicas dentales, actualmente en fase beta"

INSTRUCCIONES GENERALES:
- S√© conversacional y natural
- Haz preguntas espec√≠ficas para obtener detalles √∫tiles
- Reconoce cuando el usuario aporta informaci√≥n valiosa
- Sugiere √°reas importantes que podr√≠an faltar
- Mant√©n un tono profesional pero amigable
- Cuando el contexto est√© completo (3+ elementos o 50+ palabras), sugiere una pregunta espec√≠fica

Responde SIEMPRE en este formato JSON v√°lido:
{
  "message_type": "question|information|ready",
  "response_text": "Tu respuesta conversacional al usuario",
  "context_update": "SOLO informaci√≥n NUEVA del mensaje actual, sin repetir lo que ya est√° en el contexto",
  "suggestions": ["Sugerencia 1", "Sugerencia 2"],
  "suggested_final_question": "Pregunta espec√≠fica sugerida para las IAs principales (solo si message_type es 'ready')"
}

Tipos de message_type:
- "question": El usuario hizo una pregunta o necesita orientaci√≥n
- "information": El usuario aport√≥ informaci√≥n √∫til para el contexto
- "ready": El contexto parece completo y listo para enviar a IAs principales (incluye suggested_final_question)
"""
    
    def _build_conversation_messages(
        self,
        system_prompt: str,
        user_message: str,
        conversation_history: List[ContextMessage],
        current_context: str
    ) -> List[Dict[str, str]]:
        """Construye los mensajes para enviar a GPT-3.5."""
        messages = [{"role": "system", "content": system_prompt}]
        
        # Agregar contexto actual si existe
        if current_context.strip():
            context_info = f"""
CONTEXTO ACTUAL DEL PROYECTO:
{current_context}

---
"""
            messages.append({"role": "system", "content": context_info})
        
        # Agregar historial de conversaci√≥n (√∫ltimos 6 mensajes para no exceder l√≠mites)
        recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
        for msg in recent_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # Agregar mensaje actual del usuario
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def _update_accumulated_context(self, current_context: str, new_info: str) -> str:
        """
        Actualiza el contexto acumulado con nueva informaci√≥n.
        Evita duplicaci√≥n pero preserva informaci√≥n importante.
        """
        if not new_info or not new_info.strip():
            return current_context
            
        if not current_context or not current_context.strip():
            return new_info.strip()
        
        # Normalizar textos para comparaci√≥n
        current_lower = current_context.lower().strip()
        new_lower = new_info.lower().strip()
        
        # Si la nueva informaci√≥n es id√©ntica o est√° completamente contenida, no agregar
        if new_lower == current_lower or new_lower in current_lower:
            logger.debug(f"Informaci√≥n duplicada detectada (contenida): {new_lower[:50]}...")
            return current_context
        
        # Verificar si hay superposici√≥n significativa de palabras (>70%)
        current_words = set(current_lower.split())
        new_words = set(new_lower.split())
        
        if len(new_words) > 0:
            overlap_ratio = len(current_words.intersection(new_words)) / len(new_words)
            
            # Si hay mucha superposici√≥n, verificar si la nueva info es m√°s completa
            if overlap_ratio > 0.7:
                # Si la nueva informaci√≥n es m√°s larga y espec√≠fica, reemplazar
                if len(new_info) > len(current_context) * 1.2:
                    logger.debug(f"Reemplazando contexto con versi√≥n m√°s completa")
                    return new_info.strip()
                # Si es similar o m√°s corta, mantener la actual
                else:
                    logger.debug(f"Informaci√≥n similar detectada, manteniendo contexto actual")
                    return current_context
        
        # Detectar duplicaci√≥n sem√°ntica espec√≠fica
        # Verificar frases comunes que podr√≠an estar duplicadas
        duplicate_patterns = [
            r'startup.*software.*gesti√≥n.*cl√≠nicas.*dentales',
            r'fase.*beta',
            r'software.*gesti√≥n.*cl√≠nicas.*dentales',
            r'cl√≠nicas.*dentales.*fase.*beta'
        ]
        
        import re
        for pattern in duplicate_patterns:
            # Si el patr√≥n aparece en ambos textos, es probable duplicaci√≥n
            if re.search(pattern, current_lower) and re.search(pattern, new_lower):
                # Extraer solo las partes nuevas
                logger.debug(f"Patr√≥n duplicado detectado: {pattern}")
                
                # Intentar extraer solo informaci√≥n nueva
                new_parts = []
                new_sentences = new_info.split('.')
                
                for sentence in new_sentences:
                    sentence = sentence.strip()
                    if sentence and not any(re.search(pattern, sentence.lower()) for pattern in duplicate_patterns):
                        # Esta oraci√≥n no contiene patrones duplicados
                        if sentence.lower() not in current_lower:
                            new_parts.append(sentence)
                
                if new_parts:
                    # Combinar solo las partes nuevas
                    new_content = '. '.join(new_parts)
                    if current_context.endswith('.'):
                        return f"{current_context} {new_content.strip()}"
                    else:
                        return f"{current_context}. {new_content.strip()}"
                else:
                    # No hay partes nuevas, mantener el contexto actual
                    logger.debug("No se encontraron partes nuevas despu√©s de filtrar duplicaci√≥n")
                    return current_context
        
        # Combinar informaci√≥n complementaria si no hay duplicaci√≥n detectada
        # Usar punto para separar informaci√≥n distinta
        if current_context.endswith('.'):
            return f"{current_context} {new_info.strip()}"
        else:
            return f"{current_context}. {new_info.strip()}"
    
    def include_moderator_synthesis(self, current_context: str, synthesis_text: str, key_themes: list = None, recommendations: list = None) -> str:
        """
        Incluye la s√≠ntesis del moderador en el contexto acumulado.
        
        Args:
            current_context: Contexto actual acumulado
            synthesis_text: Texto de s√≠ntesis del moderador
            key_themes: Temas clave identificados por el moderador (opcional)
            recommendations: Recomendaciones del moderador (opcional)
            
        Returns:
            Contexto actualizado con la s√≠ntesis del moderador
        """
        if not synthesis_text.strip():
            return current_context
        
        # Evitar duplicaci√≥n - verificar si ya est√° incluida
        if "üî¨ An√°lisis del Moderador IA" in current_context:
            logger.debug("S√≠ntesis del moderador ya incluida en el contexto")
            return current_context
        
        # Crear resumen estructurado de la s√≠ntesis
        moderator_section = "## üî¨ An√°lisis del Moderador IA\n\n"
        
        # Incluir temas clave si est√°n disponibles
        if key_themes:
            moderator_section += "**Temas Clave Identificados:**\n"
            for theme in key_themes[:3]:  # Limitar a 3 temas principales
                moderator_section += f"‚Ä¢ {theme}\n"
            moderator_section += "\n"
        
        # Incluir recomendaciones si est√°n disponibles
        if recommendations:
            moderator_section += "**Recomendaciones Principales:**\n"
            for rec in recommendations[:3]:  # Limitar a 3 recomendaciones principales
                moderator_section += f"‚Ä¢ {rec}\n"
            moderator_section += "\n"
        
        # Incluir s√≠ntesis (limitada para no abrumar el contexto)
        synthesis_preview = synthesis_text[:800] + "..." if len(synthesis_text) > 800 else synthesis_text
        moderator_section += f"**S√≠ntesis del An√°lisis:**\n{synthesis_preview}"
        
        # Combinar con el contexto existente
        if current_context.strip():
            return f"{current_context}\n\n{moderator_section}"
        else:
            return moderator_section
    
    def _count_context_elements(self, context: str) -> int:
        """Cuenta los elementos de contexto separados por l√≠neas."""
        if not context.strip():
            return 0
        
        # Contar p√°rrafos no vac√≠os
        elements = [line.strip() for line in context.split('\n') if line.strip()]
        return len(elements)
    
    async def _create_fallback_response(
        self, 
        user_message: str, 
        current_context: str
    ) -> ContextChatResponse:
        """Crea una respuesta de fallback en caso de error con OpenAI."""
        
        # Usar el nuevo clasificador inteligente
        message_type, confidence = await self._smart_classify(user_message)
        is_question = message_type == "question"
        
        # Si la confianza es baja, pedir aclaraci√≥n
        if confidence < 0.55:
            return ContextChatResponse(
                session_id=uuid4(),
                ai_response="No estoy seguro de si eso es una pregunta o informaci√≥n. ¬øPodr√≠as aclararlo o aportar m√°s detalles?",
                message_type="question",
                accumulated_context=current_context,
                suggestions=["Reformula tu mensaje", "S√© m√°s espec√≠fico", "Agrega m√°s contexto"],
                context_elements_count=self._count_context_elements(current_context),
                suggested_final_question=None
            )
        
        if is_question:
            # Es una pregunta
            updated_context = current_context
            if not current_context.strip():
                response_text = "¬°Hola! Estoy aqu√≠ para ayudarte a construir el contexto de tu consulta. ¬øPodr√≠as contarme m√°s sobre tu proyecto o situaci√≥n?"
            else:
                response_text = "Entiendo tu pregunta. ¬øPodr√≠as darme m√°s detalles espec√≠ficos para poder ayudarte mejor?"
            
            suggestions = [
                "Describe tu proyecto o empresa",
                "Comparte los principales desaf√≠os",
                "Explica qu√© tipo de ayuda necesitas"
            ]
            message_type = "question"
            context_elements = self._count_context_elements(updated_context)
        else:
            # Es informaci√≥n
            updated_context = self._update_accumulated_context(current_context, user_message)
            response_text = "Perfecto, he registrado esa informaci√≥n. ¬øHay algo m√°s que quieras agregar sobre tu proyecto?"
            suggestions = [
                "Comparte m√°s detalles sobre tu negocio",
                "Describe tus principales desaf√≠os",
                "H√°blame de tus objetivos"
            ]
            message_type = "information"
            context_elements = self._count_context_elements(updated_context)
        
        # Evaluar si est√° listo para finalizar
        if self._should_suggest_finalization(updated_context):
            message_type = "ready"
            suggested_question = self._generate_suggested_question(updated_context, user_message)
            response_text = f"{response_text}\n\nüéØ **Contexto completado!** Te sugiero esta pregunta para las IAs principales: \"{suggested_question}\""
            suggestions = ["Usa la pregunta sugerida", "Modifica la pregunta si lo necesitas", "Agrega m√°s contexto si falta algo"]
        else:
            suggested_question = None
        
        return ContextChatResponse(
            session_id=uuid4(),
            ai_response=response_text,
            message_type=message_type,
            accumulated_context=updated_context,
            suggestions=suggestions,
            context_elements_count=context_elements,
            suggested_final_question=suggested_question
        )
    
    def _should_suggest_finalization(self, context: str) -> bool:
        """
        Eval√∫a si el contexto est√° listo para finalizar autom√°ticamente.
        
        Args:
            context: Contexto acumulado
            
        Returns:
            True si el contexto parece completo
        """
        if not context.strip():
            return False
        
        # Criterios para sugerir finalizaci√≥n
        element_count = self._count_context_elements(context)
        word_count = len(context.split())
        
        # Sugerir finalizaci√≥n si hay suficiente informaci√≥n
        return element_count >= 3 or word_count >= 50
    
    def _generate_suggested_question(self, context: str, last_message: str) -> str:
        """
        Genera una pregunta sugerida basada en el contexto acumulado.
        
        Args:
            context: Contexto acumulado
            last_message: √öltimo mensaje del usuario
            
        Returns:
            Pregunta sugerida para las IAs principales
        """
        # Analizar el contexto para generar una pregunta relevante
        context_lower = context.lower()
        last_message_lower = last_message.lower()
        
        # Detectar temas principales
        if any(word in context_lower for word in ['empresa', 'negocio', 'startup', 'compa√±√≠a']):
            if any(word in context_lower for word in ['marketing', 'ventas', 'clientes']):
                return "¬øC√≥mo puedo mejorar mi estrategia de marketing y ventas bas√°ndome en mi situaci√≥n actual?"
            elif any(word in context_lower for word in ['tecnolog√≠a', 'desarrollo', 'software', 'app']):
                return "¬øQu√© recomendaciones tecnol√≥gicas me dar√≠as para mi proyecto?"
            elif any(word in context_lower for word in ['financiero', 'inversi√≥n', 'dinero', 'capital']):
                return "¬øQu√© opciones financieras y de inversi√≥n ser√≠an m√°s adecuadas para mi situaci√≥n?"
            else:
                return "¬øQu√© estrategias y recomendaciones me sugieres para hacer crecer mi negocio?"
        
        elif any(word in context_lower for word in ['proyecto', 'idea', 'plan']):
            return "¬øC√≥mo puedo desarrollar y ejecutar exitosamente este proyecto?"
        
        elif any(word in context_lower for word in ['problema', 'desaf√≠o', 'dificultad']):
            return "¬øCu√°les ser√≠an las mejores soluciones para resolver estos desaf√≠os?"
        
        else:
            # Pregunta gen√©rica basada en el √∫ltimo mensaje
            if '?' in last_message:
                return last_message.strip()
            else:
                return f"¬øPuedes ayudarme con {last_message.lower().strip()}?"
    
    async def suggest_finalization(self, context: str) -> bool:
        """
        Eval√∫a si el contexto est√° listo para finalizar.
        
        Args:
            context: Contexto acumulado
            
        Returns:
            True si el contexto parece completo
        """
        if not context.strip():
            return False
        
        # Criterios simples para sugerir finalizaci√≥n
        element_count = self._count_context_elements(context)
        word_count = len(context.split())
        
        # Sugerir finalizaci√≥n si hay suficiente informaci√≥n
        return element_count >= 3 or word_count >= 50
    
    async def _simple_gpt_call(self, user_message: str, current_context: str) -> ContextChatResponse:
        """
        Llamada simplificada a GPT-3.5 cuando falla el parsing JSON.
        
        Args:
            user_message: Mensaje del usuario
            current_context: Contexto actual
            
        Returns:
            ContextChatResponse con respuesta simple
        """
        simple_prompt = f"""
Analiza este mensaje del usuario y responde si es una PREGUNTA o INFORMACI√ìN.

Mensaje: "{user_message}"
Contexto actual: "{current_context}"

Responde SOLO con este formato JSON:
{{"message_type": "question", "response_text": "Tu respuesta aqu√≠"}}

Si es informaci√≥n, usa "information" como message_type.
"""
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": simple_prompt}],
            temperature=0.1,
            max_tokens=200
        )
        
        response_content = response.choices[0].message.content.strip()
        
        # Intentar parsear JSON simple
        try:
            parsed = json.loads(response_content)
            message_type = parsed.get("message_type", "question")
            ai_response = parsed.get("response_text", "¬øPodr√≠as darme m√°s detalles?")
        except:
            # Si falla, usar el nuevo clasificador inteligente
            message_type, confidence = await self._smart_classify(user_message)
            is_question = message_type == "question"
            ai_response = "Entiendo. ¬øPodr√≠as contarme m√°s detalles?" if is_question else "Perfecto, he registrado esa informaci√≥n."
        
        # Actualizar contexto si es informaci√≥n
        updated_context = current_context
        if message_type == "information":
            updated_context = self._update_accumulated_context(current_context, user_message)
        
        # Evaluar finalizaci√≥n
        suggested_question = None
        if self._should_suggest_finalization(updated_context):
            message_type = "ready"
            suggested_question = self._generate_suggested_question(updated_context, user_message)
            ai_response = f"{ai_response}\n\nüéØ **Contexto completado!** Te sugiero esta pregunta: \"{suggested_question}\""
        
        return ContextChatResponse(
            session_id=uuid4(),
            ai_response=ai_response,
            message_type=message_type,
            accumulated_context=updated_context,
            suggestions=["Contin√∫a compartiendo informaci√≥n", "Haz preguntas espec√≠ficas"],
            context_elements_count=self._count_context_elements(updated_context),
            suggested_final_question=suggested_question
        )

# Instancia global del servicio
context_builder_service = ContextBuilderService() 